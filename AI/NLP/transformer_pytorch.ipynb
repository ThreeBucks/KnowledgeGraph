{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7222693b",
   "metadata": {},
   "source": [
    "## Reference\n",
    "[Transformer的最简洁pytorch实现](https://zhuanlan.zhihu.com/p/665148654)\n",
    "\n",
    "## Transformer网络结构\n",
    "![transformer](images/transformer_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19634bd",
   "metadata": {},
   "source": [
    "## pytorch实现\n",
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80fd4283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea19bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S: 起始标记\n",
    "# E: 结束标记\n",
    "# P: padding标记, 将当前序列补齐至最长序列长度的占位符\n",
    "sentence = [\n",
    "    # enc_input   dec_input    dec_output\n",
    "    ['ich mochte ein bier P','S i want a beer .', 'i want a beer . E'],\n",
    "    ['ich mochte ein cola P','S i want a coke .', 'i want a coke . E'],\n",
    "]\n",
    "# 词典，padding用0来表示\n",
    "# 源词典\n",
    "src_vocab = {'P':0, 'ich':1,'mochte':2,'ein':3,'bier':4,'cola':5}\n",
    "src_vocab_size = len(src_vocab) # 6\n",
    "# 目标词典（包含特殊符）\n",
    "tgt_vocab = {'P':0,'i':1,'want':2,'a':3,'beer':4,'coke':5,'S':6,'E':7,'.':8}\n",
    "# 反向映射词典，idx ——> word\n",
    "idx2word = {v:k for k, v in tgt_vocab.items()}\n",
    "tgt_vocab_size = len(tgt_vocab) # 9\n",
    "\n",
    "src_len = 5 # 输入序列enc_input的最长序列长度，其实就是最长的那句话的token数\n",
    "tgt_len = 6 # 输出序列dec_input/dec_output的最长序列长度"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2500d21d",
   "metadata": {},
   "source": [
    "### 构建DataLoader\n",
    "![dataloader](images/transformer_dataloader.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0de20c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " enc_inputs: \n",
      " tensor([[1, 2, 3, 4, 0],\n",
      "        [1, 2, 3, 5, 0]])\n",
      " dec_inputs: \n",
      " tensor([[6, 1, 2, 3, 4, 8],\n",
      "        [6, 1, 2, 3, 5, 8]])\n",
      " dec_outputs: \n",
      " tensor([[1, 2, 3, 4, 8, 7],\n",
      "        [1, 2, 3, 5, 8, 7]])\n"
     ]
    }
   ],
   "source": [
    "# 构建模型输入的Tensor\n",
    "def make_data(sentence):\n",
    "    enc_inputs, dec_inputs, dec_outputs = [], [], []\n",
    "    for i in range(len(sentence)):\n",
    "        enc_input = [src_vocab[word] for word in sentence[i][0].split()]\n",
    "        dec_input = [tgt_vocab[word] for word in sentence[i][1].split()]\n",
    "        dec_output = [tgt_vocab[word] for word in sentence[i][2].split()]\n",
    "        \n",
    "        enc_inputs.append(enc_input)\n",
    "        dec_inputs.append(dec_input)\n",
    "        dec_outputs.append(dec_output)\n",
    "    \n",
    "    # LongTensor是专用于存储整型的，Tensor则可以存浮点、整数、bool等多种类型\n",
    "    return torch.LongTensor(enc_inputs), torch.LongTensor(dec_inputs), torch.LongTensor(dec_outputs)\n",
    "\n",
    "enc_inputs, dec_inputs, dec_outputs = make_data(sentence)\n",
    "print(' enc_inputs: \\n', enc_inputs)  # enc_inputs: [2,5]\n",
    "print(' dec_inputs: \\n', dec_inputs)  # dec_inputs: [2,6]\n",
    "print(' dec_outputs: \\n', dec_outputs) # dec_outputs: [2,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84b69443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用Dataset加载数据\n",
    "class MyDataSet(Data.Dataset):\n",
    "    def __init__(self, enc_inputs, dec_inputs, dec_outputs):\n",
    "        super(MyDataSet, self).__init__()\n",
    "        self.enc_inputs = enc_inputs\n",
    "        self.dec_inputs = dec_inputs\n",
    "        self.dec_outputs = dec_outputs\n",
    "        \n",
    "    def __len__(self,):\n",
    "        # 我们前面的enc_inputs.shape = [2,5],所以这个返回的是2\n",
    "        return self.enc_inputs.shape[0]\n",
    "    \n",
    "    # 根据idx返回的是一组 enc_input, dec_input, dec_output\n",
    "    def __getitem__(self, idx):\n",
    "        return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]\n",
    "\n",
    "# 构建DataLoader\n",
    "loader = Data.DataLoader(dataset=MyDataSet(enc_inputs, dec_inputs, dec_outputs), batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d892271",
   "metadata": {},
   "source": [
    "### 模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f177ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用来表示一个词的向量长度\n",
    "d_model = 512\n",
    " \n",
    "# FFN的隐藏层神经元个数\n",
    "d_ff = 2048\n",
    " \n",
    "# 分头后的q、k、v词向量长度，依照原文我们都设为64\n",
    "# 原文：queries and kes of dimention d_k,and values of dimension d_v .所以q和k的长度都用d_k来表示\n",
    "d_k = d_v = 64\n",
    " \n",
    "# Encoder Layer 和 Decoder Layer的个数\n",
    "n_layers = 6\n",
    " \n",
    "# 多头注意力中head的个数，原文：we employ h = 8 parallel attention layers, or heads\n",
    "n_heads = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44677a72",
   "metadata": {},
   "source": [
    "### 模型细节\n",
    "- Transformer包含Encoder和Decoder\n",
    "- Encoder和Decoder各自包含6个Layer\n",
    "- Encoder Layer中包含 Self Attention 和 FFN 两个Sub Layer\n",
    "- Decoder Layer中包含 Masked Self Attention、 Cross Attention、 FFN 三个Sub Layer\n",
    "\n",
    "布局如图：\n",
    "![model detail](images/transformer_model_detail.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58ce996",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "用于为输入的词向量进行位置编码\n",
    "原文：The positional encodings have the same dimension d_model as the embeddings, so that the two can be summed\n",
    "\n",
    "![pos encoding](images/transformer_pos_encoding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5753e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000): # dropout是原文的0.1，max_len原文没找到\n",
    "        '''max_len是假设的一个句子最多包含5000个token'''\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # 开始位置编码部分,先生成一个max_len * d_model 的矩阵，即5000 * 512\n",
    "        # 5000是一个句子中最多的token数，512是一个token用多长的向量来表示，5000*512这个矩阵用于表示一个句子的信息\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # pos：[max_len,1],即[5000,1]\n",
    "        # 先把括号内的分式求出来,pos是[5000,1],分母是[256],通过广播机制相乘后是[5000,256]\n",
    "        div_term = pos / pow(10000.0, torch.arange(0, d_model, 2).float() / d_model)\n",
    "        # 再取正余弦\n",
    "        pe[:, 0::2] = torch.sin(div_term)\n",
    "        pe[:, 1::2] = torch.cos(div_term)\n",
    "        # 一个句子要做一次pe，一个batch中会有多个句子，所以增加一维用来和输入的一个batch的数据相加时做广播\n",
    "        pe = pe.unsqueeze(0)# [5000,512] -> [1,5000,512] \n",
    "        # 将pe作为固定参数保存到缓冲区，不会被更新\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''x: [batch_size, seq_len, d_model]'''\n",
    "        # 5000是我们预定义的最大的seq_len，就是说我们把最多的情况pe都算好了，用的时候用多少就取多少\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)# return: [batch_size, seq_len, d_model], 和输入的形状相同"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d656cf",
   "metadata": {},
   "source": [
    "### Pad Mask\n",
    "- 首先我们要清楚，这是一个计算mask的函数，它的返回是一个布尔矩阵，为True的位置是需要被mask掉的，False的位置是不需要动的\n",
    "- 其次这个函数是理解Transformer代码中非常重要的一环，因为我们输入模型的句子有长有短，我们用占位符P统一补足成了最长的那个句子的长度，而这些占位符是没有意义的，不能让他们吸收到query的注意力，因此我们要把这些位置设为True\n",
    "- 这个计算出的mask在何时被使用呢？\n",
    "    - 在query和key的转置相乘得出（len_q,len_k）这个注意力分数矩阵以后，将使用本函数得到的mask来掩盖相乘结果矩阵\n",
    "    - 原来的相乘结果矩阵（len_q,len_k）中第 i 行第 j 列的意义是“作为q的序列中第i个词对作为k的序列中第j个词的注意力分数”，而第 i 整行就是q中这个词对k中所有词的注意力，第 j 整列就是q中所有词对k中第j个词的注意力分数，作为padding，q中的所有词都不应该注意它，因此对应列均需设为True\n",
    "- 为什么只有k的padding位被mask了，q的padding位为什么没被mask？（即此函数的返回矩阵为什么只有最后几列是True，最后几行不应该也是True么）\n",
    "    - 按理来说是这样的，作为padding不该被别人注意，同时它也不该注意别人，计算出的padding对其他词的注意力也是无意义的，我们这里其实是偷了个懒，但这是因为：q中的padding对k中的词的注意力我们是不会用到的，因为我们不会用一个padding字符去预测下一个词，并且它的向量表示不管怎么更新都不会影响到别的q中别的词的计算，所以我们就放任自流了。但k中的padding不一样，如果不管它将无意义的吸收掉大量q中词汇的注意力，使得模型的学习出现偏差。\n",
    "    - p.s. 哈佛团队也是这么实现的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a152e808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为enc_input和dec_input做一个mask，把占位符P的token（就是0） mask掉\n",
    "# 返回一个[batch_size, len_q, len_k]大小的布尔张量，True是需要mask掉的位置\n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # seq_k.data.eq(0)返回一个等大的布尔张量，seq_k元素等于0的位置为True,否则为False\n",
    "    # 然后扩维以保证后续操作的兼容(广播)\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1) # pad_attn_mask: [batch_size,1,len_k]\n",
    "    # 要为每一个q提供一份k，所以把第二维度扩展了q次\n",
    "    # 另注意expand并非真正加倍了内存，只是重复了引用，对任意引用的修改都会修改原始值\n",
    "    # 这里是因为我们不会修改这个mask所以用它来节省内存\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k) # return: [batch_size, len_q, len_k]\n",
    "    # 返回的是batch_size个 len_q * len_k的矩阵，内容是True和False，\n",
    "    # 第i行第j列表示的是query的第i个词对key的第j个词的注意力是否无意义，若无意义则为True，有意义的为False（即被padding的位置是True）\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf545487",
   "metadata": {},
   "source": [
    "### Subsequence Mask\n",
    "此函数对应Transformer模型架构中Decoder的第一个注意力“Masked Multi-Head self Attention”中的Masked一词，作用是防止模型看到未来时刻的输入\n",
    "\n",
    "怎么理解呢？\n",
    "- 其实这需要结合实际使用模型进行预测时的数据流动来理解（可以参考本文最后一部分的测试流程图），因为实际的预测中解码器Decoder其实是一个词一个词累积着的输出的(每次比上回多一个)，每输出一个词就会把这个词拼到新的dec_input中，然后再预测下一个词，直到输出终止标记，这是测试过程。而在训练过程中，我们为了能让模型学到最精确的表示，每一回我们喂给decoder的都是完整的正确答案，即正确翻译后的句子，但是我们的模型必须遵循真实的使用场景，所以我们需要让decoder只看到当前时刻以前的输出（即正确答案不能多看，最多看到当前这个题的就行了），而实现这一目的的方法就是：屏蔽掉当前时刻以后的注意力分数，而这个时间上“以后”的概念，体现在数据上就是序列中token的前后顺序，所以当前token之后的都需要被mask。\n",
    "- 也可以另一个角度来理解，我们的dec_input是一个完整的句子，包含翻译后的所有token，self attention的形状是tgt_len × tgt_len，但是对于第i个token来说，它不应该看到第i+1个以及更后面的token，因为按理来说后面的还没生成呢，所以他不应该对这些还不存在的token计算注意力，所以：token i 对应的这一行，token i 列之后的列都需要被mask\n",
    "\n",
    "因为这个mask只用于解码器中的第一个self attention，q和k都是自己（dec_input），所以是一个方阵:\n",
    "\n",
    "![transformer_subsequence_mask](images/transformer_subsequence_mask.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e525dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于获取对后续位置的掩码，防止在预测过程中看到未来时刻的输入\n",
    "# 原文：to prevent positions from attending to subsequent positions\n",
    "def get_attn_subsequence_mask(seq):\n",
    "    \"\"\"seq: [batch_size, tgt_len]\"\"\"\n",
    "    # batch_size个 tgt_len * tgt_len的mask矩阵\n",
    "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
    "    # np.triu 是生成一个 upper triangular matrix 上三角矩阵，k是相对于主对角线的偏移量\n",
    "    # k=1意为不包含主对角线（从主对角线向上偏移1开始）\n",
    "    subsequence_mask = np.triu(np.ones(attn_shape), k=1)\n",
    "    subsequence_mask = torch.from_numpy(subsequence_mask).byte() # 因为只有0、1所以用byte节省内存\n",
    "    return subsequence_mask  # return: [batch_size, tgt_len, tgt_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7fbf42",
   "metadata": {},
   "source": [
    "### ScaledDotProductAttention\n",
    "此函数用于计算缩放点积注意力，在MultiHeadAttention中被调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d348f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductionAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductionAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        '''\n",
    "        Q: [batch_size, n_heads, len_q, d_k]\n",
    "        K: [batch_size, n_heads, len_k, d_k]\n",
    "        V: [batch_size, n_heads, len_v(=len_k), d_v] 全文两处用到注意力，一处是self attention，另一处是co attention，前者不必说，后者的k和v都是encoder的输出，所以k和v的形状总是相同的\n",
    "        attn_mask: [batch_size, n_heads, seq_len, seq_len]\n",
    "        '''\n",
    "        # 1) 计算注意力分数QK^T/sqrt(d_k)\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k)  # scores: [batch_size, n_heads, len_q, len_k]\n",
    "        # 2)  进行 mask 和 softmax\n",
    "        # mask为True的位置会被设为-1e9\n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "        attn = nn.Softmax(dim=-1)(scores)  # attn: [batch_size, n_heads, len_q, len_k]\n",
    "        # 3) 乘V得到最终的加权和\n",
    "        context = torch.matmul(attn, V)  # context: [batch_size, n_heads, len_q, d_v]\n",
    "        '''\n",
    "        scores矩阵的第i行表示q中的第i个token对k中的所有token的注意力分数，这个分数在计算最终的加权和时候对于value的不同维度(d1-d_model)是独立的，\n",
    "        即对于结果矩阵的第一行第一列来说，它是由scores矩阵的第一行(seq_q 中的 token 1 对seq_k中的所有token的注意力分数)乘以V矩阵的第一列(seq_V中所有token在维度1即d_1上的取值)得到的，\n",
    "        这个意义就是token 1考虑了所有token在当前维度的取值后通过计算对每个token注意力更新得到新的值\n",
    "\n",
    "        得出的context是每个维度(d_1-d_v)都考虑了在当前维度(这一列)当前token对所有token的注意力后更新的新的值，\n",
    "        换言之每个维度d是相互独立的，每个维度考虑自己的所有token的注意力，所以可以理解成1列扩展到多列\n",
    "\n",
    "        返回的context: [batch_size, n_heads, len_q, d_v]本质上还是batch_size个句子，\n",
    "        只不过每个句子中词向量维度512被分成了8个部分，分别由8个头各自看一部分，每个头算的是整个句子(一列)的512/8=64个维度，最后按列拼接起来\n",
    "        '''\n",
    "        return context # context: [batch_size, n_heads, len_q, d_v]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f2334d",
   "metadata": {},
   "source": [
    "### MultiHeadAttention\n",
    "多头注意力的实现，Transformer的核心\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6c3d2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.concat = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, input_Q, input_K, input_V, attn_mask):\n",
    "        '''\n",
    "        input_Q: [batch_size, len_q, d_model] len_q是作为query的句子的长度，比如enc_inputs（2,5,512）作为输入，那句子长度5就是len_q\n",
    "        input_K: [batch_size, len_k, d_model]\n",
    "        input_K: [batch_size, len_v(len_k), d_model]\n",
    "        attn_mask: [batch_size, seq_len, seq_len]\n",
    "        '''\n",
    "        residual, batch_size = input_Q, input_Q.size(0)\n",
    "\n",
    "        # 1）linear projection [batch_size, seq_len, d_model] ->  [batch_size, n_heads, seq_len, d_k/d_v]\n",
    "        Q = self.W_Q(input_Q).view(batch_size, -1, n_heads, d_k).transpose(1, 2) # Q: [batch_size, n_heads, len_q, d_k]\n",
    "        K = self.W_K(input_K).view(batch_size, -1, n_heads, d_k).transpose(1, 2) # K: [batch_size, n_heads, len_k, d_k]\n",
    "        V = self.W_V(input_V).view(batch_size, -1, n_heads, d_v).transpose(1, 2) # V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "\n",
    "        # 2）计算注意力\n",
    "        # 自我复制n_heads次，为每个头准备一份mask\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)  # attn_mask: [batch_size, n_heads, seq_len, seq_len]\n",
    "        context = ScaledDotProductionAttention()(Q, K, V, attn_mask) # context: [batch_size, n_heads, len_q, d_v]\n",
    "\n",
    "        # 3）concat部分\n",
    "        context = torch.cat([context[:,i,:,:] for i in range(context.size(1))], dim=-1)\n",
    "        output = self.concat(context)  # [batch_size, len_q, d_model]\n",
    "        return nn.LayerNorm(d_model).cuda()(output + residual)  # output: [batch_size, len_q, d_model]\n",
    "\n",
    "        '''        \n",
    "        最后的concat部分，网上的大部分实现都采用的是下面这种方式（也是哈佛NLP团队的写法）\n",
    "        context = context.transpose(1, 2).reshape(batch_size, -1, d_model)\n",
    "        output = self.linear(context)\n",
    "        但是我认为这种方式拼回去会使原来的位置乱序，于是并未采用这种写法，两种写法最终的实验结果是相近的\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e1ef25",
   "metadata": {},
   "source": [
    "### FeedForward Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c9a3841",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        # 就是一个MLP\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        '''inputs: [batch_size, seq_len, d_model]'''\n",
    "        residual = inputs\n",
    "        output = self.fc(inputs)\n",
    "        return nn.LayerNorm(d_model).cuda()(output + residual) # return： [batch_size, seq_len, d_model] 形状不变"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafba3d8",
   "metadata": {},
   "source": [
    "### Encoder Layer\n",
    "包含一个MultiHeadAttention和一个FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12f51279",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PositionwiseFeedForward()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        '''\n",
    "        enc_inputs: [batch_size, src_len, d_model]\n",
    "        enc_self_attn_mask: [batch_size, src_len, src_len]\n",
    "        '''\n",
    "        # Q、K、V均为 enc_inputs\n",
    "        enc_ouputs = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_ouputs: [batch_size, src_len, d_model]\n",
    "        enc_ouputs = self.pos_ffn(enc_ouputs) # enc_outputs: [batch_size, src_len, d_model]\n",
    "        return enc_ouputs  # enc_outputs: [batch_size, src_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1abdf9",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "包含一个源序列词向量嵌入nn.Embedding、一个位置编码PositionalEncoding和6个Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0461e46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        # 直接调的现成接口完成词向量的编码，输入是类别数和每一个类别要映射成的向量长度\n",
    "        self.src_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.pos_emb = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, enc_inputs):\n",
    "        '''enc_inputs: [batch_size, src_len]'''\n",
    "        enc_outputs = self.src_emb(enc_inputs) # [batch_size, src_len] -> [batch_size, src_len, d_model]\n",
    "        enc_outputs = self.pos_emb(enc_outputs) # enc_outputs: [batch_size, src_len, d_model]\n",
    "        # Encoder中是self attention，所以传入的Q、K都是enc_inputs\n",
    "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)  # enc_self_attn_mask: [batch_size, src_len, src_len]\n",
    "        for layer in self.layers:\n",
    "            enc_outputs = layer(enc_outputs, enc_self_attn_mask)\n",
    "        return enc_outputs  # enc_outputs: [batch_size, src_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89deccd",
   "metadata": {},
   "source": [
    "### Decoder Layer\n",
    "包含两个MultiHeadAttention和一个FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e84c88eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.dec_self_attn = MultiHeadAttention()\n",
    "        self.dec_enc_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PositionwiseFeedForward()\n",
    "\n",
    "    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
    "        '''\n",
    "        dec_inputs: [batch_size, tgt_len, d_model]\n",
    "        enc_outputs: [batch_size, src_len, d_model]\n",
    "        dec_self_attn_mask: [batch_size, tgt_len, tgt_len]\n",
    "        dec_enc_attn_mask: [batch_size, tgt_len, src_len] 前者是Q后者是K\n",
    "        '''\n",
    "        dec_outputs = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)\n",
    "        dec_outputs = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
    "        dec_outputs = self.pos_ffn(dec_outputs)\n",
    "\n",
    "        return dec_outputs # dec_outputs: [batch_size, tgt_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90fbc5a",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "包含一个目标序列词向量序列嵌入nn.Embedding、一个位置编码PositionalEncoding还有6个Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15e12474",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_emb = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "\n",
    "    def forward(self, dec_inputs, enc_inputs, enc_outputs):\n",
    "        '''\n",
    "        这三个参数对应的不是Q、K、V，dec_inputs是Q，enc_outputs是K和V，enc_inputs是用来计算padding mask的\n",
    "        dec_inputs: [batch_size, tgt_len]\n",
    "        enc_inpus: [batch_size, src_len]\n",
    "        enc_outputs: [batch_size, src_len, d_model]\n",
    "        '''\n",
    "        dec_outputs = self.tgt_emb(dec_inputs)\n",
    "        dec_outputs = self.pos_emb(dec_outputs).cuda()\n",
    "        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).cuda()\n",
    "        dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs).cuda()\n",
    "        # 将两个mask叠加，布尔值可以视为0和1，和大于0的位置是需要被mask掉的，赋为True，和为0的位置是有意义的为False\n",
    "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask +\n",
    "                                       dec_self_attn_subsequence_mask), 0).cuda()\n",
    "        # 这是co-attention部分，为啥传入的是enc_inputs而不是enc_outputs呢\n",
    "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            dec_outputs = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
    "\n",
    "        return dec_outputs # dec_outputs: [batch_size, tgt_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa55b3e4",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "包含一个Encoder、一个Decoder、一个nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50e8fd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder().cuda()\n",
    "        self.decoder = Decoder().cuda()\n",
    "        self.projection = nn.Linear(d_model, tgt_vocab_size).cuda()\n",
    "\n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        '''\n",
    "        enc_inputs: [batch_size, src_len]\n",
    "        dec_inputs: [batch_size, tgt_len]\n",
    "        '''\n",
    "        enc_outputs = self.encoder(enc_inputs)\n",
    "        dec_outputs = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
    "        dec_logits = self.projection(dec_outputs) # dec_logits: [batch_size, tgt_len, tgt_vocab_size]\n",
    "\n",
    "        # 解散batch，一个batch中有batch_size个句子，每个句子有tgt_len个词（即tgt_len行），\n",
    "        # 现在让他们按行依次排布，如前tgt_len行是第一个句子的每个词的预测概率，\n",
    "        # 再往下tgt_len行是第二个句子的，一直到batch_size * tgt_len行\n",
    "        return dec_logits.view(-1, dec_logits.size(-1))  #  [batch_size * tgt_len, tgt_vocab_size]\n",
    "        '''最后变形的原因是：nn.CrossEntropyLoss接收的输入的第二个维度必须是类别'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6a01c9",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a212e4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 2.2995078563690186\n",
      "Epoch [2/1000], Loss: 2.1870057582855225\n",
      "Epoch [3/1000], Loss: 2.0263421535491943\n",
      "Epoch [4/1000], Loss: 1.7105131149291992\n",
      "Epoch [5/1000], Loss: 1.4523123502731323\n",
      "Epoch [6/1000], Loss: 1.2780213356018066\n",
      "Epoch [7/1000], Loss: 1.1274300813674927\n",
      "Epoch [8/1000], Loss: 0.8737473487854004\n",
      "Epoch [9/1000], Loss: 0.6866509914398193\n",
      "Epoch [10/1000], Loss: 0.527069628238678\n",
      "Epoch [11/1000], Loss: 0.42862963676452637\n",
      "Epoch [12/1000], Loss: 0.3263358771800995\n",
      "Epoch [13/1000], Loss: 0.2708769142627716\n",
      "Epoch [14/1000], Loss: 0.19993896782398224\n",
      "Epoch [15/1000], Loss: 0.1575062870979309\n",
      "Epoch [16/1000], Loss: 0.14167454838752747\n",
      "Epoch [17/1000], Loss: 0.1144634559750557\n",
      "Epoch [18/1000], Loss: 0.10200963169336319\n",
      "Epoch [19/1000], Loss: 0.0760466605424881\n",
      "Epoch [20/1000], Loss: 0.06922558695077896\n",
      "Epoch [21/1000], Loss: 0.050017740577459335\n",
      "Epoch [22/1000], Loss: 0.05266401544213295\n",
      "Epoch [23/1000], Loss: 0.04071607068181038\n",
      "Epoch [24/1000], Loss: 0.036163073033094406\n",
      "Epoch [25/1000], Loss: 0.036524221301078796\n",
      "Epoch [26/1000], Loss: 0.03964049741625786\n",
      "Epoch [27/1000], Loss: 0.034615516662597656\n",
      "Epoch [28/1000], Loss: 0.0308393407613039\n",
      "Epoch [29/1000], Loss: 0.03658546507358551\n",
      "Epoch [30/1000], Loss: 0.017659930512309074\n",
      "Epoch [31/1000], Loss: 0.022116683423519135\n",
      "Epoch [32/1000], Loss: 0.015880247578024864\n",
      "Epoch [33/1000], Loss: 0.02011832781136036\n",
      "Epoch [34/1000], Loss: 0.011213134042918682\n",
      "Epoch [35/1000], Loss: 0.012491225264966488\n",
      "Epoch [36/1000], Loss: 0.013353315182030201\n",
      "Epoch [37/1000], Loss: 0.011927064508199692\n",
      "Epoch [38/1000], Loss: 0.006667703855782747\n",
      "Epoch [39/1000], Loss: 0.012452679686248302\n",
      "Epoch [40/1000], Loss: 0.010111040435731411\n",
      "Epoch [41/1000], Loss: 0.009080552496016026\n",
      "Epoch [42/1000], Loss: 0.011754588223993778\n",
      "Epoch [43/1000], Loss: 0.010155261494219303\n",
      "Epoch [44/1000], Loss: 0.004683466162532568\n",
      "Epoch [45/1000], Loss: 0.00586672080680728\n",
      "Epoch [46/1000], Loss: 0.00506055960431695\n",
      "Epoch [47/1000], Loss: 0.006416587624698877\n",
      "Epoch [48/1000], Loss: 0.003376586129888892\n",
      "Epoch [49/1000], Loss: 0.003161797532811761\n",
      "Epoch [50/1000], Loss: 0.002724234014749527\n",
      "Epoch [51/1000], Loss: 0.003306729020550847\n",
      "Epoch [52/1000], Loss: 0.0023003453388810158\n",
      "Epoch [53/1000], Loss: 0.0016281571006402373\n",
      "Epoch [54/1000], Loss: 0.0021028302144259214\n",
      "Epoch [55/1000], Loss: 0.002029772847890854\n",
      "Epoch [56/1000], Loss: 0.0021994004491716623\n",
      "Epoch [57/1000], Loss: 0.002509330166503787\n",
      "Epoch [58/1000], Loss: 0.0018056351691484451\n",
      "Epoch [59/1000], Loss: 0.0019996464252471924\n",
      "Epoch [60/1000], Loss: 0.002062818966805935\n",
      "Epoch [61/1000], Loss: 0.0025811055675148964\n",
      "Epoch [62/1000], Loss: 0.0025694784708321095\n",
      "Epoch [63/1000], Loss: 0.002746894722804427\n",
      "Epoch [64/1000], Loss: 0.002492088358849287\n",
      "Epoch [65/1000], Loss: 0.003925559110939503\n",
      "Epoch [66/1000], Loss: 0.0031191303860396147\n",
      "Epoch [67/1000], Loss: 0.0037742387503385544\n",
      "Epoch [68/1000], Loss: 0.0029984351713210344\n",
      "Epoch [69/1000], Loss: 0.0032482855021953583\n",
      "Epoch [70/1000], Loss: 0.0023051328025758266\n",
      "Epoch [71/1000], Loss: 0.005546782165765762\n",
      "Epoch [72/1000], Loss: 0.002567786257714033\n",
      "Epoch [73/1000], Loss: 0.00250628381036222\n",
      "Epoch [74/1000], Loss: 0.0024997240398079157\n",
      "Epoch [75/1000], Loss: 0.0017845844849944115\n",
      "Epoch [76/1000], Loss: 0.002349188318476081\n",
      "Epoch [77/1000], Loss: 0.002277638064697385\n",
      "Epoch [78/1000], Loss: 0.0016177386278286576\n",
      "Epoch [79/1000], Loss: 0.0015750820748507977\n",
      "Epoch [80/1000], Loss: 0.001562718185596168\n",
      "Epoch [81/1000], Loss: 0.001382951159030199\n",
      "Epoch [82/1000], Loss: 0.0005831242306157947\n",
      "Epoch [83/1000], Loss: 0.000823101494461298\n",
      "Epoch [84/1000], Loss: 0.0007998459041118622\n",
      "Epoch [85/1000], Loss: 0.000672095047775656\n",
      "Epoch [86/1000], Loss: 0.0005248170346021652\n",
      "Epoch [87/1000], Loss: 0.0005210794042795897\n",
      "Epoch [88/1000], Loss: 0.0005952188512310386\n",
      "Epoch [89/1000], Loss: 0.00038853977457620203\n",
      "Epoch [90/1000], Loss: 0.00036072576767764986\n",
      "Epoch [91/1000], Loss: 0.0004236105887684971\n",
      "Epoch [92/1000], Loss: 0.00037281421828083694\n",
      "Epoch [93/1000], Loss: 0.0004104274557903409\n",
      "Epoch [94/1000], Loss: 0.0003022297751158476\n",
      "Epoch [95/1000], Loss: 0.00025061899214051664\n",
      "Epoch [96/1000], Loss: 0.0002935162920039147\n",
      "Epoch [97/1000], Loss: 0.00022582401288673282\n",
      "Epoch [98/1000], Loss: 0.00022746853937860578\n",
      "Epoch [99/1000], Loss: 0.00021467333135660738\n",
      "Epoch [100/1000], Loss: 0.00016005853831302375\n",
      "Epoch [101/1000], Loss: 0.0001983410766115412\n",
      "Epoch [102/1000], Loss: 0.00020169251365587115\n",
      "Epoch [103/1000], Loss: 0.00018966193601954728\n",
      "Epoch [104/1000], Loss: 0.00017217680579051375\n",
      "Epoch [105/1000], Loss: 0.00019630503084044904\n",
      "Epoch [106/1000], Loss: 0.00013926334213465452\n",
      "Epoch [107/1000], Loss: 0.00012244046956766397\n",
      "Epoch [108/1000], Loss: 0.00011018307850463316\n",
      "Epoch [109/1000], Loss: 0.00015328841982409358\n",
      "Epoch [110/1000], Loss: 0.0001054001331795007\n",
      "Epoch [111/1000], Loss: 0.00013885721273254603\n",
      "Epoch [112/1000], Loss: 8.34489765111357e-05\n",
      "Epoch [113/1000], Loss: 0.00011356946924934164\n",
      "Epoch [114/1000], Loss: 0.00012406865425873548\n",
      "Epoch [115/1000], Loss: 0.00012053279351675883\n",
      "Epoch [116/1000], Loss: 9.991668775910512e-05\n",
      "Epoch [117/1000], Loss: 0.00014828368148300797\n",
      "Epoch [118/1000], Loss: 0.00010932566510746256\n",
      "Epoch [119/1000], Loss: 0.0001182687483378686\n",
      "Epoch [120/1000], Loss: 8.6416162957903e-05\n",
      "Epoch [121/1000], Loss: 0.00014355011808220297\n",
      "Epoch [122/1000], Loss: 0.00010886324889725074\n",
      "Epoch [123/1000], Loss: 7.145966083044186e-05\n",
      "Epoch [124/1000], Loss: 0.00011368258128641173\n",
      "Epoch [125/1000], Loss: 0.00017650255176704377\n",
      "Epoch [126/1000], Loss: 0.00010578445653663948\n",
      "Epoch [127/1000], Loss: 0.0001475162716815248\n",
      "Epoch [128/1000], Loss: 8.262221672339365e-05\n",
      "Epoch [129/1000], Loss: 8.664545748615637e-05\n",
      "Epoch [130/1000], Loss: 7.813436241121963e-05\n",
      "Epoch [131/1000], Loss: 0.00014277781883720309\n",
      "Epoch [132/1000], Loss: 0.00011893059127032757\n",
      "Epoch [133/1000], Loss: 0.00013639137614518404\n",
      "Epoch [134/1000], Loss: 0.00014287486555986106\n",
      "Epoch [135/1000], Loss: 0.0001090274890884757\n",
      "Epoch [136/1000], Loss: 7.569915760541335e-05\n",
      "Epoch [137/1000], Loss: 0.00011732473649317399\n",
      "Epoch [138/1000], Loss: 0.00014488797751255333\n",
      "Epoch [139/1000], Loss: 0.00010948837734758854\n",
      "Epoch [140/1000], Loss: 0.00017935106006916612\n",
      "Epoch [141/1000], Loss: 0.00023960733960848302\n",
      "Epoch [142/1000], Loss: 9.825410234043375e-05\n",
      "Epoch [143/1000], Loss: 0.00018050278595183045\n",
      "Epoch [144/1000], Loss: 0.00012630767014343292\n",
      "Epoch [145/1000], Loss: 0.000303141976473853\n",
      "Epoch [146/1000], Loss: 0.0002489543694537133\n",
      "Epoch [147/1000], Loss: 0.0002223391056759283\n",
      "Epoch [148/1000], Loss: 0.00021697906777262688\n",
      "Epoch [149/1000], Loss: 0.0002628121292218566\n",
      "Epoch [150/1000], Loss: 0.0002183206524932757\n",
      "Epoch [151/1000], Loss: 0.00019231067562941462\n",
      "Epoch [152/1000], Loss: 0.0002923838037531823\n",
      "Epoch [153/1000], Loss: 0.00023798149777576327\n",
      "Epoch [154/1000], Loss: 0.00019257266831118613\n",
      "Epoch [155/1000], Loss: 0.0004981835372745991\n",
      "Epoch [156/1000], Loss: 0.0007614062633365393\n",
      "Epoch [157/1000], Loss: 0.0003115874424111098\n",
      "Epoch [158/1000], Loss: 0.00018155797442886978\n",
      "Epoch [159/1000], Loss: 0.00043961149640381336\n",
      "Epoch [160/1000], Loss: 0.0004918001359328628\n",
      "Epoch [161/1000], Loss: 0.00024927157210186124\n",
      "Epoch [162/1000], Loss: 0.0007144840783439577\n",
      "Epoch [163/1000], Loss: 0.000249653123319149\n",
      "Epoch [164/1000], Loss: 0.0002655619755387306\n",
      "Epoch [165/1000], Loss: 0.0003186091489624232\n",
      "Epoch [166/1000], Loss: 0.0002709845139179379\n",
      "Epoch [167/1000], Loss: 0.000535947154276073\n",
      "Epoch [168/1000], Loss: 0.00046266603749245405\n",
      "Epoch [169/1000], Loss: 0.00036084186285734177\n",
      "Epoch [170/1000], Loss: 0.00048267297097481787\n",
      "Epoch [171/1000], Loss: 0.00023597640392836183\n",
      "Epoch [172/1000], Loss: 0.00022425752831622958\n",
      "Epoch [173/1000], Loss: 0.0003188578994013369\n",
      "Epoch [174/1000], Loss: 0.0002957918622996658\n",
      "Epoch [175/1000], Loss: 0.000349329668097198\n",
      "Epoch [176/1000], Loss: 0.00031339164706878364\n",
      "Epoch [177/1000], Loss: 0.00020448990107979625\n",
      "Epoch [178/1000], Loss: 0.00018861598800867796\n",
      "Epoch [179/1000], Loss: 0.0006358828395605087\n",
      "Epoch [180/1000], Loss: 0.0004513208696153015\n",
      "Epoch [181/1000], Loss: 0.00026252251700498164\n",
      "Epoch [182/1000], Loss: 0.00026702764444053173\n",
      "Epoch [183/1000], Loss: 0.000486849487060681\n",
      "Epoch [184/1000], Loss: 0.0002738226030487567\n",
      "Epoch [185/1000], Loss: 0.0003616927133407444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [186/1000], Loss: 0.0007993365288712084\n",
      "Epoch [187/1000], Loss: 0.0006489583756774664\n",
      "Epoch [188/1000], Loss: 0.0002582654415164143\n",
      "Epoch [189/1000], Loss: 0.0005019870004616678\n",
      "Epoch [190/1000], Loss: 0.0003734893398359418\n",
      "Epoch [191/1000], Loss: 0.0003190735587850213\n",
      "Epoch [192/1000], Loss: 0.00022949765843804926\n",
      "Epoch [193/1000], Loss: 0.00028836276032961905\n",
      "Epoch [194/1000], Loss: 0.0002515152737032622\n",
      "Epoch [195/1000], Loss: 0.00017027719877660275\n",
      "Epoch [196/1000], Loss: 0.00023520061222370714\n",
      "Epoch [197/1000], Loss: 0.00014907197328284383\n",
      "Epoch [198/1000], Loss: 0.00016756677359808236\n",
      "Epoch [199/1000], Loss: 0.00019973271992057562\n",
      "Epoch [200/1000], Loss: 0.000211490856599994\n",
      "Epoch [201/1000], Loss: 0.00033272209111601114\n",
      "Epoch [202/1000], Loss: 0.00031715683871880174\n",
      "Epoch [203/1000], Loss: 0.0001884133816929534\n",
      "Epoch [204/1000], Loss: 0.00015281357627827674\n",
      "Epoch [205/1000], Loss: 0.00016596016939729452\n",
      "Epoch [206/1000], Loss: 0.0001923158415593207\n",
      "Epoch [207/1000], Loss: 0.000189975427929312\n",
      "Epoch [208/1000], Loss: 0.0001847460662247613\n",
      "Epoch [209/1000], Loss: 0.0001273047091672197\n",
      "Epoch [210/1000], Loss: 0.00014374237798620015\n",
      "Epoch [211/1000], Loss: 0.00021681356884073466\n",
      "Epoch [212/1000], Loss: 0.00011323103535687551\n",
      "Epoch [213/1000], Loss: 0.00019739205890800804\n",
      "Epoch [214/1000], Loss: 0.00016505533130839467\n",
      "Epoch [215/1000], Loss: 7.078803173499182e-05\n",
      "Epoch [216/1000], Loss: 0.00010586441931081936\n",
      "Epoch [217/1000], Loss: 0.00010807766375364736\n",
      "Epoch [218/1000], Loss: 0.0001750020164763555\n",
      "Epoch [219/1000], Loss: 7.16673894203268e-05\n",
      "Epoch [220/1000], Loss: 0.00011747993266908452\n",
      "Epoch [221/1000], Loss: 0.00010477750765858218\n",
      "Epoch [222/1000], Loss: 6.1010796343907714e-05\n",
      "Epoch [223/1000], Loss: 8.410111331613734e-05\n",
      "Epoch [224/1000], Loss: 5.7582237786846235e-05\n",
      "Epoch [225/1000], Loss: 0.000164927143487148\n",
      "Epoch [226/1000], Loss: 0.00011629162327153608\n",
      "Epoch [227/1000], Loss: 0.00012295733904466033\n",
      "Epoch [228/1000], Loss: 6.397061952156946e-05\n",
      "Epoch [229/1000], Loss: 9.835491800913587e-05\n",
      "Epoch [230/1000], Loss: 5.3600757382810116e-05\n",
      "Epoch [231/1000], Loss: 8.994043309940025e-05\n",
      "Epoch [232/1000], Loss: 9.483691974310204e-05\n",
      "Epoch [233/1000], Loss: 4.825656651519239e-05\n",
      "Epoch [234/1000], Loss: 3.907879363396205e-05\n",
      "Epoch [235/1000], Loss: 5.386923658079468e-05\n",
      "Epoch [236/1000], Loss: 6.402993312804028e-05\n",
      "Epoch [237/1000], Loss: 8.656355930725113e-05\n",
      "Epoch [238/1000], Loss: 8.0139834608417e-05\n",
      "Epoch [239/1000], Loss: 0.000170596715179272\n",
      "Epoch [240/1000], Loss: 6.058345388737507e-05\n",
      "Epoch [241/1000], Loss: 3.7698478990932927e-05\n",
      "Epoch [242/1000], Loss: 8.039682870730758e-05\n",
      "Epoch [243/1000], Loss: 4.337989958003163e-05\n",
      "Epoch [244/1000], Loss: 9.08133588382043e-05\n",
      "Epoch [245/1000], Loss: 5.55677070224192e-05\n",
      "Epoch [246/1000], Loss: 7.108232966857031e-05\n",
      "Epoch [247/1000], Loss: 2.8877258955617435e-05\n",
      "Epoch [248/1000], Loss: 0.0001750010997056961\n",
      "Epoch [249/1000], Loss: 9.204488742398098e-05\n",
      "Epoch [250/1000], Loss: 3.182765794917941e-05\n",
      "Epoch [251/1000], Loss: 8.472700574202463e-05\n",
      "Epoch [252/1000], Loss: 9.030538058141246e-05\n",
      "Epoch [253/1000], Loss: 4.611975236912258e-05\n",
      "Epoch [254/1000], Loss: 6.97785580996424e-05\n",
      "Epoch [255/1000], Loss: 6.477344868471846e-05\n",
      "Epoch [256/1000], Loss: 6.33522795396857e-05\n",
      "Epoch [257/1000], Loss: 6.186303653521463e-05\n",
      "Epoch [258/1000], Loss: 4.8742920625954866e-05\n",
      "Epoch [259/1000], Loss: 7.09400192135945e-05\n",
      "Epoch [260/1000], Loss: 0.00011531629570527002\n",
      "Epoch [261/1000], Loss: 5.590451110037975e-05\n",
      "Epoch [262/1000], Loss: 3.192638905602507e-05\n",
      "Epoch [263/1000], Loss: 2.8559354177559726e-05\n",
      "Epoch [264/1000], Loss: 8.925553265726194e-05\n",
      "Epoch [265/1000], Loss: 6.719312659697607e-05\n",
      "Epoch [266/1000], Loss: 0.00017448766448069364\n",
      "Epoch [267/1000], Loss: 5.85528468945995e-05\n",
      "Epoch [268/1000], Loss: 4.125322448089719e-05\n",
      "Epoch [269/1000], Loss: 5.432448233477771e-05\n",
      "Epoch [270/1000], Loss: 3.543231650837697e-05\n",
      "Epoch [271/1000], Loss: 5.2190443966537714e-05\n",
      "Epoch [272/1000], Loss: 3.32874187733978e-05\n",
      "Epoch [273/1000], Loss: 2.7366995709598996e-05\n",
      "Epoch [274/1000], Loss: 2.5191968234139495e-05\n",
      "Epoch [275/1000], Loss: 3.368471880094148e-05\n",
      "Epoch [276/1000], Loss: 2.8529355404316448e-05\n",
      "Epoch [277/1000], Loss: 4.7203127905959263e-05\n",
      "Epoch [278/1000], Loss: 2.6423535018693656e-05\n",
      "Epoch [279/1000], Loss: 2.5191737222485244e-05\n",
      "Epoch [280/1000], Loss: 2.290734846610576e-05\n",
      "Epoch [281/1000], Loss: 4.6249337174231187e-05\n",
      "Epoch [282/1000], Loss: 3.219434438506141e-05\n",
      "Epoch [283/1000], Loss: 2.1099374862387776e-05\n",
      "Epoch [284/1000], Loss: 2.9403434382402338e-05\n",
      "Epoch [285/1000], Loss: 3.7428861105581746e-05\n",
      "Epoch [286/1000], Loss: 1.1185603398189414e-05\n",
      "Epoch [287/1000], Loss: 1.630145197850652e-05\n",
      "Epoch [288/1000], Loss: 3.7626690755132586e-05\n",
      "Epoch [289/1000], Loss: 3.683273462229408e-05\n",
      "Epoch [290/1000], Loss: 3.473745164228603e-05\n",
      "Epoch [291/1000], Loss: 4.7400357289006934e-05\n",
      "Epoch [292/1000], Loss: 1.1523424291226547e-05\n",
      "Epoch [293/1000], Loss: 2.168551873182878e-05\n",
      "Epoch [294/1000], Loss: 2.3125445295590907e-05\n",
      "Epoch [295/1000], Loss: 2.718786709010601e-05\n",
      "Epoch [296/1000], Loss: 6.159109034342691e-06\n",
      "Epoch [297/1000], Loss: 1.707630326563958e-05\n",
      "Epoch [298/1000], Loss: 2.8549067792482674e-05\n",
      "Epoch [299/1000], Loss: 1.3420769391814247e-05\n",
      "Epoch [300/1000], Loss: 1.8069680663757026e-05\n",
      "Epoch [301/1000], Loss: 7.559785899502458e-06\n",
      "Epoch [302/1000], Loss: 8.126003194774967e-06\n",
      "Epoch [303/1000], Loss: 2.4953047613962553e-05\n",
      "Epoch [304/1000], Loss: 1.2099499144824222e-05\n",
      "Epoch [305/1000], Loss: 1.0917427971435245e-05\n",
      "Epoch [306/1000], Loss: 1.708592571958434e-05\n",
      "Epoch [307/1000], Loss: 2.75557267741533e-05\n",
      "Epoch [308/1000], Loss: 2.6771085686050355e-05\n",
      "Epoch [309/1000], Loss: 1.534785951662343e-05\n",
      "Epoch [310/1000], Loss: 1.2933942343806848e-05\n",
      "Epoch [311/1000], Loss: 8.06642128736712e-06\n",
      "Epoch [312/1000], Loss: 1.5963711121003143e-05\n",
      "Epoch [313/1000], Loss: 4.3555610318435356e-05\n",
      "Epoch [314/1000], Loss: 2.0791276256204583e-05\n",
      "Epoch [315/1000], Loss: 7.2319689934374765e-06\n",
      "Epoch [316/1000], Loss: 1.7334421499981545e-05\n",
      "Epoch [317/1000], Loss: 2.1923477106611244e-05\n",
      "Epoch [318/1000], Loss: 1.228828386956593e-05\n",
      "Epoch [319/1000], Loss: 1.7493361156084575e-05\n",
      "Epoch [320/1000], Loss: 1.5059649740578607e-05\n",
      "Epoch [321/1000], Loss: 1.1265093235124368e-05\n",
      "Epoch [322/1000], Loss: 8.215424713853281e-06\n",
      "Epoch [323/1000], Loss: 1.0212093002337497e-05\n",
      "Epoch [324/1000], Loss: 6.397524430212798e-06\n",
      "Epoch [325/1000], Loss: 4.152434939896921e-06\n",
      "Epoch [326/1000], Loss: 7.4207196121278685e-06\n",
      "Epoch [327/1000], Loss: 2.83900535578141e-05\n",
      "Epoch [328/1000], Loss: 8.841223461786285e-06\n",
      "Epoch [329/1000], Loss: 7.94720745034283e-06\n",
      "Epoch [330/1000], Loss: 1.447348859073827e-05\n",
      "Epoch [331/1000], Loss: 7.957131856528576e-06\n",
      "Epoch [332/1000], Loss: 9.486954695603345e-06\n",
      "Epoch [333/1000], Loss: 1.1642560821201187e-05\n",
      "Epoch [334/1000], Loss: 4.1126991163764615e-06\n",
      "Epoch [335/1000], Loss: 1.1563045518414583e-05\n",
      "Epoch [336/1000], Loss: 1.3828019291395321e-05\n",
      "Epoch [337/1000], Loss: 4.301443368603941e-06\n",
      "Epoch [338/1000], Loss: 2.136741386493668e-05\n",
      "Epoch [339/1000], Loss: 7.698846275161486e-06\n",
      "Epoch [340/1000], Loss: 6.0299566939647775e-06\n",
      "Epoch [341/1000], Loss: 2.037397825915832e-05\n",
      "Epoch [342/1000], Loss: 6.93395440976019e-06\n",
      "Epoch [343/1000], Loss: 4.897486633126391e-06\n",
      "Epoch [344/1000], Loss: 9.149212019110564e-06\n",
      "Epoch [345/1000], Loss: 5.920669991610339e-06\n",
      "Epoch [346/1000], Loss: 6.109434707468608e-06\n",
      "Epoch [347/1000], Loss: 6.834595751570305e-06\n",
      "Epoch [348/1000], Loss: 1.0877676686504856e-05\n",
      "Epoch [349/1000], Loss: 1.23478776004049e-05\n",
      "Epoch [350/1000], Loss: 9.139259418589063e-06\n",
      "Epoch [351/1000], Loss: 1.228826295118779e-05\n",
      "Epoch [352/1000], Loss: 8.950519259087741e-06\n",
      "Epoch [353/1000], Loss: 5.523323125089519e-06\n",
      "Epoch [354/1000], Loss: 1.1205501323274802e-05\n",
      "Epoch [355/1000], Loss: 7.410768830595771e-06\n",
      "Epoch [356/1000], Loss: 1.4125981579127256e-05\n",
      "Epoch [357/1000], Loss: 8.573036211600993e-06\n",
      "Epoch [358/1000], Loss: 1.523854098195443e-05\n",
      "Epoch [359/1000], Loss: 1.0549869330134243e-05\n",
      "Epoch [360/1000], Loss: 7.182285571616376e-06\n",
      "Epoch [361/1000], Loss: 6.6458305809646845e-06\n",
      "Epoch [362/1000], Loss: 3.2583714073552983e-06\n",
      "Epoch [363/1000], Loss: 1.8605263903737068e-05\n",
      "Epoch [364/1000], Loss: 1.2516698916442692e-05\n",
      "Epoch [365/1000], Loss: 3.784867431022576e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [366/1000], Loss: 6.089557700761361e-06\n",
      "Epoch [367/1000], Loss: 5.265040726953885e-06\n",
      "Epoch [368/1000], Loss: 8.344560228579212e-06\n",
      "Epoch [369/1000], Loss: 3.725274837051984e-06\n",
      "Epoch [370/1000], Loss: 5.54319694856531e-06\n",
      "Epoch [371/1000], Loss: 8.483618330501486e-06\n",
      "Epoch [372/1000], Loss: 8.086244633886963e-06\n",
      "Epoch [373/1000], Loss: 5.9902158682234585e-06\n",
      "Epoch [374/1000], Loss: 7.51010884414427e-06\n",
      "Epoch [375/1000], Loss: 5.39417351319571e-06\n",
      "Epoch [376/1000], Loss: 8.26508676254889e-06\n",
      "Epoch [377/1000], Loss: 4.043158241984202e-06\n",
      "Epoch [378/1000], Loss: 9.665732250141446e-06\n",
      "Epoch [379/1000], Loss: 8.344556590600405e-06\n",
      "Epoch [380/1000], Loss: 4.490194896789035e-06\n",
      "Epoch [381/1000], Loss: 6.258445864659734e-06\n",
      "Epoch [382/1000], Loss: 6.606130682484945e-06\n",
      "Epoch [383/1000], Loss: 6.000098892400274e-06\n",
      "Epoch [384/1000], Loss: 4.688864919444313e-06\n",
      "Epoch [385/1000], Loss: 5.781600975751644e-06\n",
      "Epoch [386/1000], Loss: 4.331247964728391e-06\n",
      "Epoch [387/1000], Loss: 4.003406502306461e-06\n",
      "Epoch [388/1000], Loss: 8.264959433290642e-06\n",
      "Epoch [389/1000], Loss: 9.595986739441287e-06\n",
      "Epoch [390/1000], Loss: 4.31136913903174e-06\n",
      "Epoch [391/1000], Loss: 6.2384933698922396e-06\n",
      "Epoch [392/1000], Loss: 3.089489155172487e-06\n",
      "Epoch [393/1000], Loss: 7.947190169943497e-06\n",
      "Epoch [394/1000], Loss: 9.506813512416556e-06\n",
      "Epoch [395/1000], Loss: 6.0100542214058805e-06\n",
      "Epoch [396/1000], Loss: 7.569702120235888e-06\n",
      "Epoch [397/1000], Loss: 5.384205451264279e-06\n",
      "Epoch [398/1000], Loss: 5.314678219292546e-06\n",
      "Epoch [399/1000], Loss: 9.466997653362341e-06\n",
      "Epoch [400/1000], Loss: 5.612715085590025e-06\n",
      "Epoch [401/1000], Loss: 4.370980150270043e-06\n",
      "Epoch [402/1000], Loss: 4.480240022530779e-06\n",
      "Epoch [403/1000], Loss: 7.48029378883075e-06\n",
      "Epoch [404/1000], Loss: 7.410706530208699e-06\n",
      "Epoch [405/1000], Loss: 5.572948339249706e-06\n",
      "Epoch [406/1000], Loss: 4.390833510115044e-06\n",
      "Epoch [407/1000], Loss: 7.748461030132603e-06\n",
      "Epoch [408/1000], Loss: 7.321280918404227e-06\n",
      "Epoch [409/1000], Loss: 5.781604613730451e-06\n",
      "Epoch [410/1000], Loss: 4.7087346501939464e-06\n",
      "Epoch [411/1000], Loss: 6.88422005623579e-06\n",
      "Epoch [412/1000], Loss: 4.549790901364759e-06\n",
      "Epoch [413/1000], Loss: 6.059763563825982e-06\n",
      "Epoch [414/1000], Loss: 5.225267614150653e-06\n",
      "Epoch [415/1000], Loss: 7.053140052448725e-06\n",
      "Epoch [416/1000], Loss: 3.864347945636837e-06\n",
      "Epoch [417/1000], Loss: 4.559719855024014e-06\n",
      "Epoch [418/1000], Loss: 5.17557282364578e-06\n",
      "Epoch [419/1000], Loss: 7.619315965712303e-06\n",
      "Epoch [420/1000], Loss: 9.168992619379424e-06\n",
      "Epoch [421/1000], Loss: 6.109417881816626e-06\n",
      "Epoch [422/1000], Loss: 1.0668882168829441e-05\n",
      "Epoch [423/1000], Loss: 5.324590802047169e-06\n",
      "Epoch [424/1000], Loss: 5.741840595874237e-06\n",
      "Epoch [425/1000], Loss: 1.3420481991488487e-05\n",
      "Epoch [426/1000], Loss: 5.235192929831101e-06\n",
      "Epoch [427/1000], Loss: 4.579563665174646e-06\n",
      "Epoch [428/1000], Loss: 1.0072736586153042e-05\n",
      "Epoch [429/1000], Loss: 4.867644292971818e-06\n",
      "Epoch [430/1000], Loss: 4.09282529290067e-06\n",
      "Epoch [431/1000], Loss: 6.317962743196404e-06\n",
      "Epoch [432/1000], Loss: 5.453769972518785e-06\n",
      "Epoch [433/1000], Loss: 4.649110451282468e-06\n",
      "Epoch [434/1000], Loss: 5.900760243093828e-06\n",
      "Epoch [435/1000], Loss: 1.0638977073540445e-05\n",
      "Epoch [436/1000], Loss: 3.923949861928122e-06\n",
      "Epoch [437/1000], Loss: 5.106085609440925e-06\n",
      "Epoch [438/1000], Loss: 8.145813808368985e-06\n",
      "Epoch [439/1000], Loss: 5.125946699990891e-06\n",
      "Epoch [440/1000], Loss: 3.884200395987136e-06\n",
      "Epoch [441/1000], Loss: 6.466985723818652e-06\n",
      "Epoch [442/1000], Loss: 4.48020955445827e-06\n",
      "Epoch [443/1000], Loss: 5.404106559581123e-06\n",
      "Epoch [444/1000], Loss: 6.705429314024514e-06\n",
      "Epoch [445/1000], Loss: 3.71532337339886e-06\n",
      "Epoch [446/1000], Loss: 3.4173169751738897e-06\n",
      "Epoch [447/1000], Loss: 4.112668193556601e-06\n",
      "Epoch [448/1000], Loss: 2.4636501620989293e-06\n",
      "Epoch [449/1000], Loss: 5.622622211376438e-06\n",
      "Epoch [450/1000], Loss: 4.212020940030925e-06\n",
      "Epoch [451/1000], Loss: 3.3279084163950756e-06\n",
      "Epoch [452/1000], Loss: 2.3345080535364104e-06\n",
      "Epoch [453/1000], Loss: 7.013364665908739e-06\n",
      "Epoch [454/1000], Loss: 2.264968316012528e-06\n",
      "Epoch [455/1000], Loss: 4.957068540534237e-06\n",
      "Epoch [456/1000], Loss: 5.125963070895523e-06\n",
      "Epoch [457/1000], Loss: 4.082875875610625e-06\n",
      "Epoch [458/1000], Loss: 3.89414935852983e-06\n",
      "Epoch [459/1000], Loss: 3.6557312341756187e-06\n",
      "Epoch [460/1000], Loss: 8.87086116563296e-06\n",
      "Epoch [461/1000], Loss: 5.592843081103638e-06\n",
      "Epoch [462/1000], Loss: 9.744917406351306e-06\n",
      "Epoch [463/1000], Loss: 4.609397819876904e-06\n",
      "Epoch [464/1000], Loss: 5.702104317606427e-06\n",
      "Epoch [465/1000], Loss: 1.1662062206596602e-05\n",
      "Epoch [466/1000], Loss: 9.973487067327369e-06\n",
      "Epoch [467/1000], Loss: 5.225294444244355e-06\n",
      "Epoch [468/1000], Loss: 7.321318207687e-06\n",
      "Epoch [469/1000], Loss: 4.808071480510989e-06\n",
      "Epoch [470/1000], Loss: 6.089467206038535e-06\n",
      "Epoch [471/1000], Loss: 6.218631824594922e-06\n",
      "Epoch [472/1000], Loss: 7.37094478608924e-06\n",
      "Epoch [473/1000], Loss: 8.543041076336522e-06\n",
      "Epoch [474/1000], Loss: 2.443779294480919e-06\n",
      "Epoch [475/1000], Loss: 3.2385048598371213e-06\n",
      "Epoch [476/1000], Loss: 7.569692115794169e-06\n",
      "Epoch [477/1000], Loss: 6.923974069650285e-06\n",
      "Epoch [478/1000], Loss: 4.708683263743296e-06\n",
      "Epoch [479/1000], Loss: 4.6193154048523866e-06\n",
      "Epoch [480/1000], Loss: 1.3499681699613575e-05\n",
      "Epoch [481/1000], Loss: 3.794809572355007e-06\n",
      "Epoch [482/1000], Loss: 5.930598035774892e-06\n",
      "Epoch [483/1000], Loss: 8.930484000302386e-06\n",
      "Epoch [484/1000], Loss: 7.172278856160119e-06\n",
      "Epoch [485/1000], Loss: 3.506702569211484e-06\n",
      "Epoch [486/1000], Loss: 3.0596838769270107e-06\n",
      "Epoch [487/1000], Loss: 3.894145720551023e-06\n",
      "Epoch [488/1000], Loss: 1.6391228427892202e-06\n",
      "Epoch [489/1000], Loss: 5.900775704503758e-06\n",
      "Epoch [490/1000], Loss: 5.016670456825523e-06\n",
      "Epoch [491/1000], Loss: 9.466838491789531e-06\n",
      "Epoch [492/1000], Loss: 4.4901639739691745e-06\n",
      "Epoch [493/1000], Loss: 5.443847840069793e-06\n",
      "Epoch [494/1000], Loss: 5.9206358855590224e-06\n",
      "Epoch [495/1000], Loss: 5.5630530368944164e-06\n",
      "Epoch [496/1000], Loss: 5.473584678838961e-06\n",
      "Epoch [497/1000], Loss: 5.433868864201941e-06\n",
      "Epoch [498/1000], Loss: 3.6457950045587495e-06\n",
      "Epoch [499/1000], Loss: 5.413997769210255e-06\n",
      "Epoch [500/1000], Loss: 3.0497521947836503e-06\n",
      "Epoch [501/1000], Loss: 4.698782959167147e-06\n",
      "Epoch [502/1000], Loss: 2.6523921405896544e-06\n",
      "Epoch [503/1000], Loss: 5.940542905591428e-06\n",
      "Epoch [504/1000], Loss: 6.705400664941408e-06\n",
      "Epoch [505/1000], Loss: 7.311413355637342e-06\n",
      "Epoch [506/1000], Loss: 2.4239134290837683e-06\n",
      "Epoch [507/1000], Loss: 5.2550717555277515e-06\n",
      "Epoch [508/1000], Loss: 4.132567028136691e-06\n",
      "Epoch [509/1000], Loss: 2.195431534346426e-06\n",
      "Epoch [510/1000], Loss: 3.8146681617945433e-06\n",
      "Epoch [511/1000], Loss: 4.589490799844498e-06\n",
      "Epoch [512/1000], Loss: 4.18221361542237e-06\n",
      "Epoch [513/1000], Loss: 3.2682989967725007e-06\n",
      "Epoch [514/1000], Loss: 5.831267571920762e-06\n",
      "Epoch [515/1000], Loss: 3.4768982004607096e-06\n",
      "Epoch [516/1000], Loss: 7.748339157842565e-06\n",
      "Epoch [517/1000], Loss: 3.3875141980388435e-06\n",
      "Epoch [518/1000], Loss: 6.3577176661056e-06\n",
      "Epoch [519/1000], Loss: 2.761665427897242e-06\n",
      "Epoch [520/1000], Loss: 2.2749011350242654e-06\n",
      "Epoch [521/1000], Loss: 3.2782220387161942e-06\n",
      "Epoch [522/1000], Loss: 4.05308901463286e-06\n",
      "Epoch [523/1000], Loss: 3.6755970995727694e-06\n",
      "Epoch [524/1000], Loss: 9.347779268864542e-06\n",
      "Epoch [525/1000], Loss: 4.678923687606584e-06\n",
      "Epoch [526/1000], Loss: 3.7352081108110724e-06\n",
      "Epoch [527/1000], Loss: 2.9404757242446067e-06\n",
      "Epoch [528/1000], Loss: 4.142494162806543e-06\n",
      "Epoch [529/1000], Loss: 4.1722778405528516e-06\n",
      "Epoch [530/1000], Loss: 5.602747478405945e-06\n",
      "Epoch [531/1000], Loss: 7.927127626317088e-06\n",
      "Epoch [532/1000], Loss: 2.602726226541563e-06\n",
      "Epoch [533/1000], Loss: 4.003396043117391e-06\n",
      "Epoch [534/1000], Loss: 5.483543645823374e-06\n",
      "Epoch [535/1000], Loss: 1.778200044100231e-06\n",
      "Epoch [536/1000], Loss: 3.894135261361953e-06\n",
      "Epoch [537/1000], Loss: 2.374242512814817e-06\n",
      "Epoch [538/1000], Loss: 4.6888558244972955e-06\n",
      "Epoch [539/1000], Loss: 2.3643087843083777e-06\n",
      "Epoch [540/1000], Loss: 4.192152118775994e-06\n",
      "Epoch [541/1000], Loss: 4.897420240013162e-06\n",
      "Epoch [542/1000], Loss: 4.132558842684375e-06\n",
      "Epoch [543/1000], Loss: 2.711996785365045e-06\n",
      "Epoch [544/1000], Loss: 4.470278781809611e-06\n",
      "Epoch [545/1000], Loss: 4.967018412571633e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [546/1000], Loss: 5.304758815327659e-06\n",
      "Epoch [547/1000], Loss: 4.8378537940152455e-06\n",
      "Epoch [548/1000], Loss: 4.5398419388220645e-06\n",
      "Epoch [549/1000], Loss: 4.231880666338839e-06\n",
      "Epoch [550/1000], Loss: 2.2152969449962256e-06\n",
      "Epoch [551/1000], Loss: 2.8709471280308207e-06\n",
      "Epoch [552/1000], Loss: 3.7749293824163033e-06\n",
      "Epoch [553/1000], Loss: 9.427099030290265e-06\n",
      "Epoch [554/1000], Loss: 6.0398174355214e-06\n",
      "Epoch [555/1000], Loss: 6.0597285482799634e-06\n",
      "Epoch [556/1000], Loss: 3.407371195862652e-06\n",
      "Epoch [557/1000], Loss: 3.1093513825908303e-06\n",
      "Epoch [558/1000], Loss: 3.5166447105439147e-06\n",
      "Epoch [559/1000], Loss: 3.079555199292372e-06\n",
      "Epoch [560/1000], Loss: 2.4040443804551614e-06\n",
      "Epoch [561/1000], Loss: 3.5563818983064266e-06\n",
      "Epoch [562/1000], Loss: 3.6755957353307167e-06\n",
      "Epoch [563/1000], Loss: 4.172283752268413e-06\n",
      "Epoch [564/1000], Loss: 6.327880782919237e-06\n",
      "Epoch [565/1000], Loss: 3.5663085782289272e-06\n",
      "Epoch [566/1000], Loss: 4.1424787013966125e-06\n",
      "Epoch [567/1000], Loss: 2.23515712605149e-06\n",
      "Epoch [568/1000], Loss: 5.6723220041021705e-06\n",
      "Epoch [569/1000], Loss: 3.61598540621344e-06\n",
      "Epoch [570/1000], Loss: 3.0398234684980707e-06\n",
      "Epoch [571/1000], Loss: 2.3322647393797524e-05\n",
      "Epoch [572/1000], Loss: 5.145799605088541e-06\n",
      "Epoch [573/1000], Loss: 2.3444395083060954e-06\n",
      "Epoch [574/1000], Loss: 3.178897713951301e-06\n",
      "Epoch [575/1000], Loss: 3.4868573948187986e-06\n",
      "Epoch [576/1000], Loss: 1.3212342082624673e-06\n",
      "Epoch [577/1000], Loss: 3.8543998925888445e-06\n",
      "Epoch [578/1000], Loss: 3.3775838801375357e-06\n",
      "Epoch [579/1000], Loss: 2.6126635930268094e-06\n",
      "Epoch [580/1000], Loss: 3.149087433484965e-06\n",
      "Epoch [581/1000], Loss: 2.9802088192809606e-06\n",
      "Epoch [582/1000], Loss: 3.2981035928969504e-06\n",
      "Epoch [583/1000], Loss: 3.5563673463911982e-06\n",
      "Epoch [584/1000], Loss: 2.13582666219736e-06\n",
      "Epoch [585/1000], Loss: 7.281554189830786e-06\n",
      "Epoch [586/1000], Loss: 4.957073997502448e-06\n",
      "Epoch [587/1000], Loss: 3.2881664537853794e-06\n",
      "Epoch [588/1000], Loss: 7.927164006105158e-06\n",
      "Epoch [589/1000], Loss: 4.480228653847007e-06\n",
      "Epoch [590/1000], Loss: 2.2947688194108196e-06\n",
      "Epoch [591/1000], Loss: 1.4702455928272684e-06\n",
      "Epoch [592/1000], Loss: 2.8808801744162338e-06\n",
      "Epoch [593/1000], Loss: 2.3742406938254135e-06\n",
      "Epoch [594/1000], Loss: 4.2815549932129215e-06\n",
      "Epoch [595/1000], Loss: 2.5729198114277096e-06\n",
      "Epoch [596/1000], Loss: 3.973609182139626e-06\n",
      "Epoch [597/1000], Loss: 2.5331864890176803e-06\n",
      "Epoch [598/1000], Loss: 1.9967480966442963e-06\n",
      "Epoch [599/1000], Loss: 2.3941129256854765e-06\n",
      "Epoch [600/1000], Loss: 2.3643044642085442e-06\n",
      "Epoch [601/1000], Loss: 6.19878301222343e-06\n",
      "Epoch [602/1000], Loss: 1.6987295339276898e-06\n",
      "Epoch [603/1000], Loss: 3.8444763958978e-06\n",
      "Epoch [604/1000], Loss: 2.2749015897716163e-06\n",
      "Epoch [605/1000], Loss: 3.427234332775697e-06\n",
      "Epoch [606/1000], Loss: 2.5431211270188214e-06\n",
      "Epoch [607/1000], Loss: 4.1822072489594575e-06\n",
      "Epoch [608/1000], Loss: 3.8444627534772735e-06\n",
      "Epoch [609/1000], Loss: 2.861012717403355e-06\n",
      "Epoch [610/1000], Loss: 2.8709455364150926e-06\n",
      "Epoch [611/1000], Loss: 4.182220891379984e-06\n",
      "Epoch [612/1000], Loss: 9.039734322868753e-06\n",
      "Epoch [613/1000], Loss: 3.804720108746551e-06\n",
      "Epoch [614/1000], Loss: 2.642461367940996e-06\n",
      "Epoch [615/1000], Loss: 2.2749031813873444e-06\n",
      "Epoch [616/1000], Loss: 3.2981063213810557e-06\n",
      "Epoch [617/1000], Loss: 2.2053652628528653e-06\n",
      "Epoch [618/1000], Loss: 2.0066847810085164e-06\n",
      "Epoch [619/1000], Loss: 3.7947977489238838e-06\n",
      "Epoch [620/1000], Loss: 4.470291969482787e-06\n",
      "Epoch [621/1000], Loss: 2.2649692255072296e-06\n",
      "Epoch [622/1000], Loss: 3.586184220694122e-06\n",
      "Epoch [623/1000], Loss: 2.1060247945570154e-06\n",
      "Epoch [624/1000], Loss: 3.029884510397096e-06\n",
      "Epoch [625/1000], Loss: 4.817993612959981e-06\n",
      "Epoch [626/1000], Loss: 2.632531504787039e-06\n",
      "Epoch [627/1000], Loss: 5.7616998674348e-06\n",
      "Epoch [628/1000], Loss: 2.384176013947581e-06\n",
      "Epoch [629/1000], Loss: 2.8311908408795716e-06\n",
      "Epoch [630/1000], Loss: 5.086220880912151e-06\n",
      "Epoch [631/1000], Loss: 2.3643026452191407e-06\n",
      "Epoch [632/1000], Loss: 2.7417970613896614e-06\n",
      "Epoch [633/1000], Loss: 1.639123752283922e-06\n",
      "Epoch [634/1000], Loss: 4.619332230504369e-06\n",
      "Epoch [635/1000], Loss: 2.6921325115836225e-06\n",
      "Epoch [636/1000], Loss: 1.768266542967467e-06\n",
      "Epoch [637/1000], Loss: 1.867608261818532e-06\n",
      "Epoch [638/1000], Loss: 2.5927872684405884e-06\n",
      "Epoch [639/1000], Loss: 3.2583654956397368e-06\n",
      "Epoch [640/1000], Loss: 1.698727828625124e-06\n",
      "Epoch [641/1000], Loss: 2.4338455659744795e-06\n",
      "Epoch [642/1000], Loss: 3.0994226563052507e-06\n",
      "Epoch [643/1000], Loss: 4.867637926508905e-06\n",
      "Epoch [644/1000], Loss: 2.3643071926926496e-06\n",
      "Epoch [645/1000], Loss: 2.5828501293290174e-06\n",
      "Epoch [646/1000], Loss: 2.3146374132920755e-06\n",
      "Epoch [647/1000], Loss: 2.7020669222110882e-06\n",
      "Epoch [648/1000], Loss: 3.437181703702663e-06\n",
      "Epoch [649/1000], Loss: 2.3345044155576034e-06\n",
      "Epoch [650/1000], Loss: 4.043145509058377e-06\n",
      "Epoch [651/1000], Loss: 3.784856062338804e-06\n",
      "Epoch [652/1000], Loss: 2.5133206236205297e-06\n",
      "Epoch [653/1000], Loss: 1.6689245967427269e-06\n",
      "Epoch [654/1000], Loss: 3.854406259051757e-06\n",
      "Epoch [655/1000], Loss: 2.3941138351801783e-06\n",
      "Epoch [656/1000], Loss: 2.18549712371896e-06\n",
      "Epoch [657/1000], Loss: 1.4503766578854993e-06\n",
      "Epoch [658/1000], Loss: 2.960352276204503e-06\n",
      "Epoch [659/1000], Loss: 1.6192565226447186e-06\n",
      "Epoch [660/1000], Loss: 1.4305077229437302e-06\n",
      "Epoch [661/1000], Loss: 2.4239141112047946e-06\n",
      "Epoch [662/1000], Loss: 2.652397597557865e-06\n",
      "Epoch [663/1000], Loss: 2.1755647594545735e-06\n",
      "Epoch [664/1000], Loss: 3.9736100916343275e-06\n",
      "Epoch [665/1000], Loss: 2.175564532080898e-06\n",
      "Epoch [666/1000], Loss: 4.748453193315072e-06\n",
      "Epoch [667/1000], Loss: 3.3974440611928003e-06\n",
      "Epoch [668/1000], Loss: 1.7483990859545884e-06\n",
      "Epoch [669/1000], Loss: 1.5894530633886461e-06\n",
      "Epoch [670/1000], Loss: 2.5033843940036604e-06\n",
      "Epoch [671/1000], Loss: 2.2351689494826132e-06\n",
      "Epoch [672/1000], Loss: 1.6589928009125288e-06\n",
      "Epoch [673/1000], Loss: 2.4437842967017787e-06\n",
      "Epoch [674/1000], Loss: 3.288168727522134e-06\n",
      "Epoch [675/1000], Loss: 1.8874749230235466e-06\n",
      "Epoch [676/1000], Loss: 2.7119995138491504e-06\n",
      "Epoch [677/1000], Loss: 5.970342044747667e-06\n",
      "Epoch [678/1000], Loss: 1.9073436305916402e-06\n",
      "Epoch [679/1000], Loss: 1.8874758325182484e-06\n",
      "Epoch [680/1000], Loss: 2.9504165013349848e-06\n",
      "Epoch [681/1000], Loss: 3.4967652027262375e-06\n",
      "Epoch [682/1000], Loss: 3.695449549923069e-06\n",
      "Epoch [683/1000], Loss: 2.6523948690737598e-06\n",
      "Epoch [684/1000], Loss: 2.612659272926976e-06\n",
      "Epoch [685/1000], Loss: 4.6689979171787854e-06\n",
      "Epoch [686/1000], Loss: 1.817937231862743e-06\n",
      "Epoch [687/1000], Loss: 3.764989969567978e-06\n",
      "Epoch [688/1000], Loss: 1.7384631973982323e-06\n",
      "Epoch [689/1000], Loss: 2.8212627967150183e-06\n",
      "Epoch [690/1000], Loss: 2.175562030970468e-06\n",
      "Epoch [691/1000], Loss: 2.1258940705592977e-06\n",
      "Epoch [692/1000], Loss: 4.539825113170082e-06\n",
      "Epoch [693/1000], Loss: 2.086157564917812e-06\n",
      "Epoch [694/1000], Loss: 2.5331867163913557e-06\n",
      "Epoch [695/1000], Loss: 2.1159546577109722e-06\n",
      "Epoch [696/1000], Loss: 3.4471195249352604e-06\n",
      "Epoch [697/1000], Loss: 1.907344994833693e-06\n",
      "Epoch [698/1000], Loss: 1.9768831407418475e-06\n",
      "Epoch [699/1000], Loss: 2.443782932459726e-06\n",
      "Epoch [700/1000], Loss: 1.8378062804913498e-06\n",
      "Epoch [701/1000], Loss: 2.7418047920946265e-06\n",
      "Epoch [702/1000], Loss: 2.9802206427120836e-06\n",
      "Epoch [703/1000], Loss: 2.6325224098400213e-06\n",
      "Epoch [704/1000], Loss: 1.7086625803131028e-06\n",
      "Epoch [705/1000], Loss: 2.1954308522253996e-06\n",
      "Epoch [706/1000], Loss: 3.168965804434265e-06\n",
      "Epoch [707/1000], Loss: 2.0762236090376973e-06\n",
      "Epoch [708/1000], Loss: 2.9901541438448476e-06\n",
      "Epoch [709/1000], Loss: 2.394113607806503e-06\n",
      "Epoch [710/1000], Loss: 3.8643288462481e-06\n",
      "Epoch [711/1000], Loss: 1.4702454791404307e-06\n",
      "Epoch [712/1000], Loss: 2.145761754945852e-06\n",
      "Epoch [713/1000], Loss: 3.1888364446786e-06\n",
      "Epoch [714/1000], Loss: 3.188827804478933e-06\n",
      "Epoch [715/1000], Loss: 2.930536766143632e-06\n",
      "Epoch [716/1000], Loss: 2.52325480687432e-06\n",
      "Epoch [717/1000], Loss: 2.314636049050023e-06\n",
      "Epoch [718/1000], Loss: 1.519913894298952e-06\n",
      "Epoch [719/1000], Loss: 1.9768835954891983e-06\n",
      "Epoch [720/1000], Loss: 1.8874767420129501e-06\n",
      "Epoch [721/1000], Loss: 1.7086634898078046e-06\n",
      "Epoch [722/1000], Loss: 1.4205747902451549e-06\n",
      "Epoch [723/1000], Loss: 3.2384966743848054e-06\n",
      "Epoch [724/1000], Loss: 2.543124537623953e-06\n",
      "Epoch [725/1000], Loss: 3.655732825791347e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [726/1000], Loss: 1.9768806396314176e-06\n",
      "Epoch [727/1000], Loss: 1.5993888382581645e-06\n",
      "Epoch [728/1000], Loss: 2.284839183630538e-06\n",
      "Epoch [729/1000], Loss: 1.450377567380201e-06\n",
      "Epoch [730/1000], Loss: 1.798070229597215e-06\n",
      "Epoch [731/1000], Loss: 1.8775414218907827e-06\n",
      "Epoch [732/1000], Loss: 4.351105872046901e-06\n",
      "Epoch [733/1000], Loss: 1.8974097883983632e-06\n",
      "Epoch [734/1000], Loss: 2.6623322355590062e-06\n",
      "Epoch [735/1000], Loss: 3.854401256830897e-06\n",
      "Epoch [736/1000], Loss: 2.9504215035558445e-06\n",
      "Epoch [737/1000], Loss: 1.5099809616003768e-06\n",
      "Epoch [738/1000], Loss: 1.798068637981487e-06\n",
      "Epoch [739/1000], Loss: 3.864351583615644e-06\n",
      "Epoch [740/1000], Loss: 2.5729243589012185e-06\n",
      "Epoch [741/1000], Loss: 1.688795123300224e-06\n",
      "Epoch [742/1000], Loss: 2.8113317966926843e-06\n",
      "Epoch [743/1000], Loss: 1.7583333828952163e-06\n",
      "Epoch [744/1000], Loss: 2.821270754793659e-06\n",
      "Epoch [745/1000], Loss: 2.384180106673739e-06\n",
      "Epoch [746/1000], Loss: 1.8676095123737468e-06\n",
      "Epoch [747/1000], Loss: 2.255035951748141e-06\n",
      "Epoch [748/1000], Loss: 1.7384650163876358e-06\n",
      "Epoch [749/1000], Loss: 1.788133545232995e-06\n",
      "Epoch [750/1000], Loss: 2.155696847694344e-06\n",
      "Epoch [751/1000], Loss: 2.0563545604090905e-06\n",
      "Epoch [752/1000], Loss: 1.7384655848218245e-06\n",
      "Epoch [753/1000], Loss: 2.3444401904271217e-06\n",
      "Epoch [754/1000], Loss: 2.17556043935474e-06\n",
      "Epoch [755/1000], Loss: 2.016619646383333e-06\n",
      "Epoch [756/1000], Loss: 3.486855575829395e-06\n",
      "Epoch [757/1000], Loss: 2.4040427888394333e-06\n",
      "Epoch [758/1000], Loss: 1.629190251151158e-06\n",
      "Epoch [759/1000], Loss: 3.2484349503647536e-06\n",
      "Epoch [760/1000], Loss: 3.675591187857208e-06\n",
      "Epoch [761/1000], Loss: 2.0762242911587236e-06\n",
      "Epoch [762/1000], Loss: 1.3212346630098182e-06\n",
      "Epoch [763/1000], Loss: 3.914021817763569e-06\n",
      "Epoch [764/1000], Loss: 3.2782390917418525e-06\n",
      "Epoch [765/1000], Loss: 2.692133193704649e-06\n",
      "Epoch [766/1000], Loss: 2.4139737888617674e-06\n",
      "Epoch [767/1000], Loss: 1.7682673387753312e-06\n",
      "Epoch [768/1000], Loss: 1.8974097883983632e-06\n",
      "Epoch [769/1000], Loss: 2.066290335278609e-06\n",
      "Epoch [770/1000], Loss: 1.897407855722122e-06\n",
      "Epoch [771/1000], Loss: 2.5530589482514188e-06\n",
      "Epoch [772/1000], Loss: 1.7185976730615948e-06\n",
      "Epoch [773/1000], Loss: 1.7980682969209738e-06\n",
      "Epoch [774/1000], Loss: 2.38418056142109e-06\n",
      "Epoch [775/1000], Loss: 1.5000463235992356e-06\n",
      "Epoch [776/1000], Loss: 2.1457619823195273e-06\n",
      "Epoch [777/1000], Loss: 2.9305365387699567e-06\n",
      "Epoch [778/1000], Loss: 2.294769046784495e-06\n",
      "Epoch [779/1000], Loss: 2.3941083782119676e-06\n",
      "Epoch [780/1000], Loss: 2.4636476609884994e-06\n",
      "Epoch [781/1000], Loss: 2.1656276203430025e-06\n",
      "Epoch [782/1000], Loss: 2.523256398490048e-06\n",
      "Epoch [783/1000], Loss: 3.6656476822827244e-06\n",
      "Epoch [784/1000], Loss: 2.1358248432079563e-06\n",
      "Epoch [785/1000], Loss: 1.658991436670476e-06\n",
      "Epoch [786/1000], Loss: 1.6987291928671766e-06\n",
      "Epoch [787/1000], Loss: 1.877542104011809e-06\n",
      "Epoch [788/1000], Loss: 1.3311688462636084e-06\n",
      "Epoch [789/1000], Loss: 1.7583328144610277e-06\n",
      "Epoch [790/1000], Loss: 3.0795606562605826e-06\n",
      "Epoch [791/1000], Loss: 1.5000479152149637e-06\n",
      "Epoch [792/1000], Loss: 1.132487000177207e-06\n",
      "Epoch [793/1000], Loss: 3.0100243293418316e-06\n",
      "Epoch [794/1000], Loss: 1.837804916249297e-06\n",
      "Epoch [795/1000], Loss: 2.483519210727536e-06\n",
      "Epoch [796/1000], Loss: 2.3643103759241058e-06\n",
      "Epoch [797/1000], Loss: 2.1656260287272744e-06\n",
      "Epoch [798/1000], Loss: 2.7418038825999247e-06\n",
      "Epoch [799/1000], Loss: 3.0298722322186222e-06\n",
      "Epoch [800/1000], Loss: 1.927212679220247e-06\n",
      "Epoch [801/1000], Loss: 1.7583332692083786e-06\n",
      "Epoch [802/1000], Loss: 2.106025704051717e-06\n",
      "Epoch [803/1000], Loss: 1.3013664101890754e-06\n",
      "Epoch [804/1000], Loss: 1.3907729226048104e-06\n",
      "Epoch [805/1000], Loss: 2.314639232281479e-06\n",
      "Epoch [806/1000], Loss: 1.7483999954492901e-06\n",
      "Epoch [807/1000], Loss: 4.1226280700357165e-06\n",
      "Epoch [808/1000], Loss: 1.3709042150367168e-06\n",
      "Epoch [809/1000], Loss: 1.5596524463035166e-06\n",
      "Epoch [810/1000], Loss: 1.9272117697255453e-06\n",
      "Epoch [811/1000], Loss: 2.4239136564574437e-06\n",
      "Epoch [812/1000], Loss: 2.4139769720932236e-06\n",
      "Epoch [813/1000], Loss: 1.3808386256641825e-06\n",
      "Epoch [814/1000], Loss: 1.9570156837289687e-06\n",
      "Epoch [815/1000], Loss: 1.5099818710950785e-06\n",
      "Epoch [816/1000], Loss: 1.4305087461252697e-06\n",
      "Epoch [817/1000], Loss: 1.9073405610470218e-06\n",
      "Epoch [818/1000], Loss: 1.738465357448149e-06\n",
      "Epoch [819/1000], Loss: 2.3941138351801783e-06\n",
      "Epoch [820/1000], Loss: 1.9570145468605915e-06\n",
      "Epoch [821/1000], Loss: 1.3411030295173987e-06\n",
      "Epoch [822/1000], Loss: 1.9371432244952302e-06\n",
      "Epoch [823/1000], Loss: 3.248434040870052e-06\n",
      "Epoch [824/1000], Loss: 2.513318804631126e-06\n",
      "Epoch [825/1000], Loss: 2.274902954013669e-06\n",
      "Epoch [826/1000], Loss: 2.056355924651143e-06\n",
      "Epoch [827/1000], Loss: 2.1954328985884786e-06\n",
      "Epoch [828/1000], Loss: 1.4007067647980875e-06\n",
      "Epoch [829/1000], Loss: 1.5199158269751933e-06\n",
      "Epoch [830/1000], Loss: 1.4801798897678964e-06\n",
      "Epoch [831/1000], Loss: 2.602726226541563e-06\n",
      "Epoch [832/1000], Loss: 1.062948172148026e-06\n",
      "Epoch [833/1000], Loss: 1.7185976730615948e-06\n",
      "Epoch [834/1000], Loss: 1.8974111526404158e-06\n",
      "Epoch [835/1000], Loss: 1.3709046697840677e-06\n",
      "Epoch [836/1000], Loss: 2.4537177978345426e-06\n",
      "Epoch [837/1000], Loss: 2.5729275421326747e-06\n",
      "Epoch [838/1000], Loss: 2.7914713882637443e-06\n",
      "Epoch [839/1000], Loss: 1.9967517346231034e-06\n",
      "Epoch [840/1000], Loss: 2.1954267594992416e-06\n",
      "Epoch [841/1000], Loss: 1.63912534389965e-06\n",
      "Epoch [842/1000], Loss: 2.165631030948134e-06\n",
      "Epoch [843/1000], Loss: 2.0066854631295428e-06\n",
      "Epoch [844/1000], Loss: 1.3510365306501626e-06\n",
      "Epoch [845/1000], Loss: 1.3013665238759131e-06\n",
      "Epoch [846/1000], Loss: 1.7583338376425672e-06\n",
      "Epoch [847/1000], Loss: 2.2252356757235248e-06\n",
      "Epoch [848/1000], Loss: 2.5927940896508517e-06\n",
      "Epoch [849/1000], Loss: 1.5993887245713267e-06\n",
      "Epoch [850/1000], Loss: 1.6391244344049483e-06\n",
      "Epoch [851/1000], Loss: 3.3676367365842452e-06\n",
      "Epoch [852/1000], Loss: 1.8378068489255384e-06\n",
      "Epoch [853/1000], Loss: 2.9305463158380007e-06\n",
      "Epoch [854/1000], Loss: 1.6887952369870618e-06\n",
      "Epoch [855/1000], Loss: 1.3112991155139753e-06\n",
      "Epoch [856/1000], Loss: 2.721935516092344e-06\n",
      "Epoch [857/1000], Loss: 1.9073446537731797e-06\n",
      "Epoch [858/1000], Loss: 1.8080041854773299e-06\n",
      "Epoch [859/1000], Loss: 1.3907728089179727e-06\n",
      "Epoch [860/1000], Loss: 2.5729232220328413e-06\n",
      "Epoch [861/1000], Loss: 1.927213133967598e-06\n",
      "Epoch [862/1000], Loss: 1.3113007071297034e-06\n",
      "Epoch [863/1000], Loss: 2.6325271846872056e-06\n",
      "Epoch [864/1000], Loss: 1.1225530442970921e-06\n",
      "Epoch [865/1000], Loss: 2.3146342300606193e-06\n",
      "Epoch [866/1000], Loss: 1.4007073332322761e-06\n",
      "Epoch [867/1000], Loss: 2.5530537186568836e-06\n",
      "Epoch [868/1000], Loss: 1.6093223393909284e-06\n",
      "Epoch [869/1000], Loss: 2.5927956812665798e-06\n",
      "Epoch [870/1000], Loss: 1.5397840797959361e-06\n",
      "Epoch [871/1000], Loss: 1.8477393268767628e-06\n",
      "Epoch [872/1000], Loss: 2.7517369289853377e-06\n",
      "Epoch [873/1000], Loss: 3.9636893234273884e-06\n",
      "Epoch [874/1000], Loss: 1.3808377161694807e-06\n",
      "Epoch [875/1000], Loss: 1.5596515368088149e-06\n",
      "Epoch [876/1000], Loss: 2.4040480184339685e-06\n",
      "Epoch [877/1000], Loss: 1.529850237602659e-06\n",
      "Epoch [878/1000], Loss: 1.9570150016079424e-06\n",
      "Epoch [879/1000], Loss: 1.9073413568548858e-06\n",
      "Epoch [880/1000], Loss: 1.6589929145993665e-06\n",
      "Epoch [881/1000], Loss: 1.8278720972375595e-06\n",
      "Epoch [882/1000], Loss: 1.2218937399666174e-06\n",
      "Epoch [883/1000], Loss: 1.927212224472896e-06\n",
      "Epoch [884/1000], Loss: 1.0232121212538914e-06\n",
      "Epoch [885/1000], Loss: 1.7980697748498642e-06\n",
      "Epoch [886/1000], Loss: 1.9570150016079424e-06\n",
      "Epoch [887/1000], Loss: 2.3047068680170923e-06\n",
      "Epoch [888/1000], Loss: 2.135827799065737e-06\n",
      "Epoch [889/1000], Loss: 1.589454768691212e-06\n",
      "Epoch [890/1000], Loss: 1.2318279232204077e-06\n",
      "Epoch [891/1000], Loss: 1.8378050299361348e-06\n",
      "Epoch [892/1000], Loss: 1.6093230215119547e-06\n",
      "Epoch [893/1000], Loss: 1.5497186041102395e-06\n",
      "Epoch [894/1000], Loss: 1.4801790939600323e-06\n",
      "Epoch [895/1000], Loss: 1.4305096556199715e-06\n",
      "Epoch [896/1000], Loss: 1.4901141867085244e-06\n",
      "Epoch [897/1000], Loss: 1.5298504649763345e-06\n",
      "Epoch [898/1000], Loss: 2.255036861242843e-06\n",
      "Epoch [899/1000], Loss: 1.3609709412776283e-06\n",
      "Epoch [900/1000], Loss: 2.404048245807644e-06\n",
      "Epoch [901/1000], Loss: 1.4503779084407142e-06\n",
      "Epoch [902/1000], Loss: 1.5199133258647635e-06\n",
      "Epoch [903/1000], Loss: 2.1556868432526244e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [904/1000], Loss: 2.473585027473746e-06\n",
      "Epoch [905/1000], Loss: 1.7682674524621689e-06\n",
      "Epoch [906/1000], Loss: 2.7418047920946265e-06\n",
      "Epoch [907/1000], Loss: 1.3212346630098182e-06\n",
      "Epoch [908/1000], Loss: 2.0662876067945035e-06\n",
      "Epoch [909/1000], Loss: 2.2947724573896267e-06\n",
      "Epoch [910/1000], Loss: 1.1225525895497412e-06\n",
      "Epoch [911/1000], Loss: 2.3047039121593116e-06\n",
      "Epoch [912/1000], Loss: 1.7583312228452996e-06\n",
      "Epoch [913/1000], Loss: 2.215300128227682e-06\n",
      "Epoch [914/1000], Loss: 2.115959205184481e-06\n",
      "Epoch [915/1000], Loss: 1.7185979004352703e-06\n",
      "Epoch [916/1000], Loss: 1.7782022041501477e-06\n",
      "Epoch [917/1000], Loss: 1.3808390804115334e-06\n",
      "Epoch [918/1000], Loss: 1.70866371718148e-06\n",
      "Epoch [919/1000], Loss: 1.8378046888756217e-06\n",
      "Epoch [920/1000], Loss: 1.4901137319611735e-06\n",
      "Epoch [921/1000], Loss: 1.380838853037858e-06\n",
      "Epoch [922/1000], Loss: 2.1358289359341143e-06\n",
      "Epoch [923/1000], Loss: 4.440481006895425e-06\n",
      "Epoch [924/1000], Loss: 2.185498487961013e-06\n",
      "Epoch [925/1000], Loss: 1.996751052502077e-06\n",
      "Epoch [926/1000], Loss: 1.6589932556598797e-06\n",
      "Epoch [927/1000], Loss: 1.8477409184924909e-06\n",
      "Epoch [928/1000], Loss: 1.7086636034946423e-06\n",
      "Epoch [929/1000], Loss: 2.2351684947352624e-06\n",
      "Epoch [930/1000], Loss: 1.3411017789621837e-06\n",
      "Epoch [931/1000], Loss: 1.5596509683746262e-06\n",
      "Epoch [932/1000], Loss: 2.4934545308497036e-06\n",
      "Epoch [933/1000], Loss: 1.857674760685768e-06\n",
      "Epoch [934/1000], Loss: 1.8179384824179579e-06\n",
      "Epoch [935/1000], Loss: 1.976882913368172e-06\n",
      "Epoch [936/1000], Loss: 1.9073451085205306e-06\n",
      "Epoch [937/1000], Loss: 1.688795578047575e-06\n",
      "Epoch [938/1000], Loss: 1.5695856063757674e-06\n",
      "Epoch [939/1000], Loss: 1.4007072195454384e-06\n",
      "Epoch [940/1000], Loss: 1.509980847913539e-06\n",
      "Epoch [941/1000], Loss: 1.8576729416963644e-06\n",
      "Epoch [942/1000], Loss: 1.7483972669651848e-06\n",
      "Epoch [943/1000], Loss: 1.8378062804913498e-06\n",
      "Epoch [944/1000], Loss: 2.0166198737570085e-06\n",
      "Epoch [945/1000], Loss: 2.98021882372268e-06\n",
      "Epoch [946/1000], Loss: 2.4139826564351097e-06\n",
      "Epoch [947/1000], Loss: 1.6788611674201093e-06\n",
      "Epoch [948/1000], Loss: 1.7782023178369855e-06\n",
      "Epoch [949/1000], Loss: 1.917279178087483e-06\n",
      "Epoch [950/1000], Loss: 1.9868173239956377e-06\n",
      "Epoch [951/1000], Loss: 1.4305096556199715e-06\n",
      "Epoch [952/1000], Loss: 1.5397827155538835e-06\n",
      "Epoch [953/1000], Loss: 2.523252760511241e-06\n",
      "Epoch [954/1000], Loss: 2.493453166607651e-06\n",
      "Epoch [955/1000], Loss: 1.2417618791005225e-06\n",
      "Epoch [956/1000], Loss: 1.8278730067322613e-06\n",
      "Epoch [957/1000], Loss: 1.6589930282862042e-06\n",
      "Epoch [958/1000], Loss: 1.8974109252667404e-06\n",
      "Epoch [959/1000], Loss: 2.314638777534128e-06\n",
      "Epoch [960/1000], Loss: 1.996751052502077e-06\n",
      "Epoch [961/1000], Loss: 1.6987293065540143e-06\n",
      "Epoch [962/1000], Loss: 1.5596524463035166e-06\n",
      "Epoch [963/1000], Loss: 2.016617600020254e-06\n",
      "Epoch [964/1000], Loss: 2.5729252683959203e-06\n",
      "Epoch [965/1000], Loss: 1.5397841934827738e-06\n",
      "Epoch [966/1000], Loss: 1.16228943625174e-06\n",
      "Epoch [967/1000], Loss: 1.0132779380001011e-06\n",
      "Epoch [968/1000], Loss: 1.4305095419331337e-06\n",
      "Epoch [969/1000], Loss: 2.354378011659719e-06\n",
      "Epoch [970/1000], Loss: 2.0563554699037923e-06\n",
      "Epoch [971/1000], Loss: 1.5993888382581645e-06\n",
      "Epoch [972/1000], Loss: 1.6788582115623285e-06\n",
      "Epoch [973/1000], Loss: 1.5000463235992356e-06\n",
      "Epoch [974/1000], Loss: 1.0828165386556066e-06\n",
      "Epoch [975/1000], Loss: 1.321234776696656e-06\n",
      "Epoch [976/1000], Loss: 4.251774498698069e-06\n",
      "Epoch [977/1000], Loss: 1.7285320836890605e-06\n",
      "Epoch [978/1000], Loss: 3.0000901460880414e-06\n",
      "Epoch [979/1000], Loss: 3.357702553330455e-06\n",
      "Epoch [980/1000], Loss: 1.8080040717904922e-06\n",
      "Epoch [981/1000], Loss: 1.8378068489255384e-06\n",
      "Epoch [982/1000], Loss: 1.8278714151165332e-06\n",
      "Epoch [983/1000], Loss: 1.4702459338877816e-06\n",
      "Epoch [984/1000], Loss: 1.8080039581036544e-06\n",
      "Epoch [985/1000], Loss: 1.5298492144211195e-06\n",
      "Epoch [986/1000], Loss: 2.1854966689716093e-06\n",
      "Epoch [987/1000], Loss: 2.056354333035415e-06\n",
      "Epoch [988/1000], Loss: 1.3510365306501626e-06\n",
      "Epoch [989/1000], Loss: 1.0728820143413031e-06\n",
      "Epoch [990/1000], Loss: 2.245103360110079e-06\n",
      "Epoch [991/1000], Loss: 1.2516962897279882e-06\n",
      "Epoch [992/1000], Loss: 1.3311687325767707e-06\n",
      "Epoch [993/1000], Loss: 1.3808390804115334e-06\n",
      "Epoch [994/1000], Loss: 2.006686145250569e-06\n",
      "Epoch [995/1000], Loss: 2.115960114679183e-06\n",
      "Epoch [996/1000], Loss: 1.8179376866100938e-06\n",
      "Epoch [997/1000], Loss: 1.6689255062374286e-06\n",
      "Epoch [998/1000], Loss: 1.7980701159103774e-06\n",
      "Epoch [999/1000], Loss: 1.9470815004751785e-06\n",
      "Epoch [1000/1000], Loss: 1.8775417629512958e-06\n"
     ]
    }
   ],
   "source": [
    "model = Transformer().cuda()\n",
    "model.train()\n",
    "# 损失函数,忽略为0的类别不对其计算loss（因为是padding无意义）\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.99)\n",
    "\n",
    "# 训练开始\n",
    "for epoch in range(1000):\n",
    "    for enc_inputs, dec_inputs, dec_outputs in loader:\n",
    "        '''\n",
    "        enc_inputs: [batch_size, src_len] [2,5]\n",
    "        dec_inputs: [batch_size, tgt_len] [2,6]\n",
    "        dec_outputs: [batch_size, tgt_len] [2,6]\n",
    "        '''\n",
    "        enc_inputs, dec_inputs, dec_outputs = enc_inputs.cuda(), dec_inputs.cuda(), dec_outputs.cuda()\n",
    "        outputs = model(enc_inputs, dec_inputs) # outputs: [batch_size * tgt_len, tgt_vocab_size]\n",
    "        # outputs: [batch_size * tgt_len, tgt_vocab_size], dec_outputs: [batch_size, tgt_len]\n",
    "        loss = criterion(outputs, dec_outputs.view(-1))  # 将dec_outputs展平成一维张量\n",
    "\n",
    "        # 更新权重\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/1000], Loss: {loss.item()}')\n",
    "\n",
    "torch.save(model, 'MyTransformer.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a6c75b",
   "metadata": {},
   "source": [
    "### 测试\n",
    "模型测试流程:\n",
    "![transformer test pipeline](images/transformer_test_pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b9d30e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(5, device='cuda:0')\n",
      "tensor(8, device='cuda:0')\n",
      "tensor([1, 2, 3, 5, 0], device='cuda:0') -> ['i', 'want', 'a', 'coke', '.']\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(8, device='cuda:0')\n",
      "tensor([1, 2, 3, 4, 0], device='cuda:0') -> ['i', 'want', 'a', 'beer', '.']\n"
     ]
    }
   ],
   "source": [
    "# 原文使用的是大小为4的beam search，这里为简单起见使用更简单的greedy贪心策略生成预测，不考虑候选，每一步选择概率最大的作为输出\n",
    "# 如果不使用greedy_decoder，那么我们之前实现的model只会进行一次预测得到['i']，并不会自回归，所以我们利用编写好的Encoder-Decoder来手动实现自回归（把上一次Decoder的输出作为下一次的输入，直到预测出终止符）\n",
    "def greedy_decoder(model, enc_input, start_symbol):\n",
    "    \"\"\"enc_input: [1, seq_len] 对应一句话\"\"\"\n",
    "    enc_outputs = model.encoder(enc_input) # enc_outputs: [1, seq_len, 512]\n",
    "    # 生成一个1行0列的，和enc_inputs.data类型相同的空张量，待后续填充\n",
    "    dec_input = torch.zeros(1, 0).type_as(enc_input.data) # .data避免影响梯度信息\n",
    "    next_symbol = start_symbol\n",
    "    flag = True\n",
    "    while flag:\n",
    "        # dec_input.detach() 创建 dec_input 的一个分离副本\n",
    "        # 生成了一个 只含有next_symbol的（1,1）的张量\n",
    "        # -1 表示在最后一个维度上进行拼接cat\n",
    "        # 这行代码的作用是将next_symbol拼接到dec_input中，作为新一轮decoder的输入\n",
    "        dec_input = torch.cat([dec_input.detach(), torch.tensor([[next_symbol]], dtype=enc_input.dtype).cuda()], -1) # dec_input: [1,当前词数]\n",
    "        dec_outputs = model.decoder(dec_input, enc_input, enc_outputs) # dec_outputs: [1, tgt_len, d_model]\n",
    "        projected = model.projection(dec_outputs) # projected: [1, 当前生成的tgt_len, tgt_vocab_size]\n",
    "        # max返回的是一个元组（最大值，最大值对应的索引），所以用[1]取到最大值对应的索引, 索引就是类别，即预测出的下一个词\n",
    "        # keepdim为False会导致减少一维\n",
    "        prob = projected.squeeze(0).max(dim=-1, keepdim=False)[1] # prob: [1],\n",
    "        # prob是一个一维的列表，包含目前为止依次生成的词的索引，最后一个是新生成的（即下一个词的类别）\n",
    "        # 因为注意力是依照前面的词算出来的，所以后生成的不会改变之前生成的\n",
    "        next_symbol = prob.data[-1]\n",
    "        if next_symbol == tgt_vocab['.']:\n",
    "            flag = False\n",
    "        print(next_symbol)\n",
    "    return dec_input  # dec_input: [1,tgt_len]\n",
    "\n",
    "\n",
    "# 测试\n",
    "model = torch.load('MyTransformer.pth')\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # 手动从loader中取一个batch的数据\n",
    "    enc_inputs, _, _ = next(iter(loader))\n",
    "    enc_inputs = enc_inputs.cuda()\n",
    "    for i in range(len(enc_inputs)):\n",
    "        greedy_dec_input = greedy_decoder(model, enc_inputs[i].view(1, -1), start_symbol=tgt_vocab['S'])\n",
    "        predict  = model(enc_inputs[i].view(1, -1), greedy_dec_input) # predict: [batch_size * tgt_len, tgt_vocab_size]\n",
    "        predict = predict.data.max(dim=-1, keepdim=False)[1]\n",
    "        '''greedy_dec_input是基于贪婪策略生成的，而贪婪解码的输出是基于当前时间步生成的假设的输出。这意味着它可能不是最优的输出，因为它仅考虑了每个时间步的最有可能的单词，而没有考虑全局上下文。\n",
    "        因此，为了获得更好的性能评估，通常会将整个输入序列和之前的假设输出序列传递给模型，以考虑全局上下文并允许模型更准确地生成输出\n",
    "        '''\n",
    "        print(enc_inputs[i], '->', [idx2word[n.item()] for n in predict])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddd8798",
   "metadata": {},
   "source": [
    "### 附1：对注意力分头的探究"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02bc5c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11],\n",
      "         [ 12,  13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23],\n",
      "         [ 24,  25,  26,  27,  28,  29,  30,  31,  32,  33,  34,  35],\n",
      "         [ 36,  37,  38,  39,  40,  41,  42,  43,  44,  45,  46,  47],\n",
      "         [ 48,  49,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59]],\n",
      "\n",
      "        [[ 60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71],\n",
      "         [ 72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83],\n",
      "         [ 84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95],\n",
      "         [ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107],\n",
      "         [108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]]])\n",
      "------------------\n",
      "tensor([[[[  0,   1,   2,   3],\n",
      "          [ 12,  13,  14,  15],\n",
      "          [ 24,  25,  26,  27],\n",
      "          [ 36,  37,  38,  39],\n",
      "          [ 48,  49,  50,  51]],\n",
      "\n",
      "         [[  4,   5,   6,   7],\n",
      "          [ 16,  17,  18,  19],\n",
      "          [ 28,  29,  30,  31],\n",
      "          [ 40,  41,  42,  43],\n",
      "          [ 52,  53,  54,  55]],\n",
      "\n",
      "         [[  8,   9,  10,  11],\n",
      "          [ 20,  21,  22,  23],\n",
      "          [ 32,  33,  34,  35],\n",
      "          [ 44,  45,  46,  47],\n",
      "          [ 56,  57,  58,  59]]],\n",
      "\n",
      "\n",
      "        [[[ 60,  61,  62,  63],\n",
      "          [ 72,  73,  74,  75],\n",
      "          [ 84,  85,  86,  87],\n",
      "          [ 96,  97,  98,  99],\n",
      "          [108, 109, 110, 111]],\n",
      "\n",
      "         [[ 64,  65,  66,  67],\n",
      "          [ 76,  77,  78,  79],\n",
      "          [ 88,  89,  90,  91],\n",
      "          [100, 101, 102, 103],\n",
      "          [112, 113, 114, 115]],\n",
      "\n",
      "         [[ 68,  69,  70,  71],\n",
      "          [ 80,  81,  82,  83],\n",
      "          [ 92,  93,  94,  95],\n",
      "          [104, 105, 106, 107],\n",
      "          [116, 117, 118, 119]]]])\n",
      "torch.Size([2, 3, 5, 4])\n",
      "------------------\n",
      "tensor([[[  0,  12,  24,  36,  48,   1,  13,  25,  37,  49,   2,  14],\n",
      "         [ 26,  38,  50,   3,  15,  27,  39,  51,   4,  16,  28,  40],\n",
      "         [ 52,   5,  17,  29,  41,  53,   6,  18,  30,  42,  54,   7],\n",
      "         [ 19,  31,  43,  55,   8,  20,  32,  44,  56,   9,  21,  33],\n",
      "         [ 45,  57,  10,  22,  34,  46,  58,  11,  23,  35,  47,  59]],\n",
      "\n",
      "        [[ 60,  72,  84,  96, 108,  61,  73,  85,  97, 109,  62,  74],\n",
      "         [ 86,  98, 110,  63,  75,  87,  99, 111,  64,  76,  88, 100],\n",
      "         [112,  65,  77,  89, 101, 113,  66,  78,  90, 102, 114,  67],\n",
      "         [ 79,  91, 103, 115,  68,  80,  92, 104, 116,  69,  81,  93],\n",
      "         [105, 117,  70,  82,  94, 106, 118,  71,  83,  95, 107, 119]]])\n",
      "torch.Size([2, 5, 12])\n",
      "------------------\n",
      "tensor([[[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11],\n",
      "         [ 12,  13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23],\n",
      "         [ 24,  25,  26,  27,  28,  29,  30,  31,  32,  33,  34,  35],\n",
      "         [ 36,  37,  38,  39,  40,  41,  42,  43,  44,  45,  46,  47],\n",
      "         [ 48,  49,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59]],\n",
      "\n",
      "        [[ 60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71],\n",
      "         [ 72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83],\n",
      "         [ 84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95],\n",
      "         [ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107],\n",
      "         [108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]]])\n",
      "torch.Size([2, 5, 12])\n"
     ]
    }
   ],
   "source": [
    "# 探究一下多头注意力从(batch_size, seq_len, d_model) 到 (batch_size,n_heads, seq_len, d_k/v)的意义\n",
    "\n",
    "# 1、这是初始的q\n",
    "q = torch.arange(120).reshape(2,5,12)\n",
    "print(q)\n",
    "print('------------------')\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 12\n",
    "n_heads = 3\n",
    "d_k = 4\n",
    "\n",
    "# 2、分成n_heads个头\n",
    "new_q = q.view(batch_size, -1, n_heads, d_k).transpose(1,2)\n",
    "# 上面一行代码的形状变化：(2,5,12) -> (2,5,3,4) -> (2,3,5,4)\n",
    "# 意义变化：最初是batch_size为2，一个batch中有2个句子，一个句子包含5个词，每个词由长度为12的向量表示\n",
    "# 最后仍然是batch_size为2，但一个batch中有3个头，每个头包含一个句子，每个句子包含5个词，但每个词由长度为4的向量表示\n",
    "\n",
    "print(new_q)\n",
    "print(new_q.shape) # torch.Size([2, 3, 5, 4])\n",
    "print('------------------')\n",
    "\n",
    "# 3、将n_heads个头合并\n",
    "final_q = q.transpose(1,2).contiguous().view(batch_size, -1, d_model)\n",
    "print(final_q)\n",
    "print(final_q.shape)\n",
    "print('------------------')\n",
    "\n",
    "# 按原来的concat实现拼回去元素顺序和最初不同了，因此改成下面这种实现\n",
    "final_q2 = torch.cat([new_q[:,i,:,:] for i in range(new_q.size(1))], dim=-1)\n",
    "print(final_q2)\n",
    "print(final_q2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ae56be",
   "metadata": {},
   "source": [
    "### 附2：模型中的数据流动\n",
    "![data flow](images/transformer_data_flow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b65535",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
