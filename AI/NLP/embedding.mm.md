# Embedding
实际就是数据之间的映射关系
## 参考文章
- 原理: https://zhuanlan.zhihu.com/p/164502624
- 详解: https://blog.csdn.net/qq_41775769/article/details/121825668
- [深入浅出Word2Vec原理解析](https://zhuanlan.zhihu.com/p/114538417)
## WHY
1. 神经网络不能处理字符数据,需要对文本预处理进行数值编码
2. 如果直接对字符从0-N进行索引编码,数字之间就存在大小及顺序关系,不合理
3. 就有了one-hot编码
4. one-hot有严重的问题,就引入了embeeding编码
## 编码方式
### one-hot
- 优势: 稀疏矩阵,计算简单
- 劣势: 
    - 硬编码,特征没有语义信息
    - 特征维度高, 且每个特征独立,会导致维度灾难
### word embedding
- 广义理解
    - 可以对数据进行降维和升维, 将数据映射到一定的维度进行观测
    - 可学习的(找到一个合适的角度去观测,获取最有用的信息)
- 词向量角度理解
    - 将高维空间(词典大小)映射到维数低很多的空间(词向量维度)
    - 每个词或词组被映射为实数域上的向量(?)

#### word2vec
TODO
##### 原理
1. skip-gram: 用一个词语作为输入,预测它周围的上下文
2. cbow: 拿一个词的上下文作为输入,预测这个词本身
#### glove